{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5ff2926",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-21T19:26:05.939263Z",
     "iopub.status.busy": "2022-06-21T19:26:05.938493Z",
     "iopub.status.idle": "2022-06-21T19:26:12.225449Z",
     "shell.execute_reply": "2022-06-21T19:26:12.224241Z"
    },
    "papermill": {
     "duration": 6.29853,
     "end_time": "2022-06-21T19:26:12.228412",
     "exception": false,
     "start_time": "2022-06-21T19:26:05.929882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import json\n",
    "import _pickle as cPickle\n",
    "from numpy import load\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, MaxPooling1D, BatchNormalization, GlobalAveragePooling1D,Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow \n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow_addons.optimizers import extend_with_decoupled_weight_decay\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "from sklearn.metrics import accuracy_score\n",
    "np_config.enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5b0bcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T19:26:12.242351Z",
     "iopub.status.busy": "2022-06-21T19:26:12.241852Z",
     "iopub.status.idle": "2022-06-21T19:26:29.456487Z",
     "shell.execute_reply": "2022-06-21T19:26:29.455462Z"
    },
    "papermill": {
     "duration": 17.22389,
     "end_time": "2022-06-21T19:26:29.458886",
     "exception": false,
     "start_time": "2022-06-21T19:26:12.234996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'closed': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], 'open': [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]}\n",
      "(88044, 475)\n",
      "(146741, 475)\n",
      "(146741,)\n",
      "[[ 1.  1.  1. ...  0.  0.  0.]\n",
      " [ 1. -1. -1. ...  0.  0.  0.]\n",
      " [ 1.  1.  1. ...  0.  0.  0.]\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [ 1. -1.  1. ...  0.  0.  0.]]\n",
      "Data dimensions:\n",
      "X: Training data's shape :  (41962, 475)\n",
      "X: Validating data's shape :  (10790, 475)\n",
      "X: Testing data's shape :  (7194, 475)\n",
      "X: open data's shape :  (86795, 475)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def Exchange_classes(c1,c2,y):\n",
    "  for i in range(len(y)):\n",
    "    if y[i]==c1:\n",
    "      y[i]=c2\n",
    "      \n",
    "    elif y[i]==c2:\n",
    "      y[i]=c1\n",
    "\n",
    "\n",
    "def balance_dataset(X,y):\n",
    "  Classes=int(np.max(y))+1\n",
    "  k=len(y)//Classes-1\n",
    "  X_=list(np.zeros(k*Classes))\n",
    "  y_=list(np.zeros(k*Classes))\n",
    "  \n",
    "  \n",
    "  for j in range(Classes):\n",
    "    for i in range(k):\n",
    "      y_[i*Classes+j]=j\n",
    "      Index=list(y).index(j)\n",
    "      X_[i*Classes+j]=X[Index]\n",
    "      y[Index]=1000\n",
    "  \n",
    "  y=np.array(y_)\n",
    "  X=np.array(X_)\n",
    "  return X,y\n",
    "  \n",
    "def Filter_close_set(Split_Number,data):\n",
    "  closed_set=data[Split_Number][\"closed\"]\n",
    "  open_set=data[Split_Number][\"open\"]\n",
    "  X_5=[]\n",
    "  X_open=[]\n",
    "  y_5=[]\n",
    "  y_open=[]\n",
    "  for i in range(len(y)):\n",
    "    if y[i] in closed_set:\n",
    "      y_5.append(closed_set.index(y[i]))\n",
    "      X_5.append(X[i])\n",
    "    else:\n",
    "      y_open.append(40)\n",
    "      X_open.append(X[i])\n",
    "      \n",
    "  return X_5,X_open,y_5,y_open\n",
    "  \n",
    "  \n",
    "f = open('../input/sydney-data/IOT_data_file.json')\n",
    "data = json.load(f)\n",
    "print(data[\"1\"])  \n",
    "\n",
    "\n",
    "dataset_dir = \"../input/sydney-data/dataset/\"\n",
    "X_train = np.load(dataset_dir+'X_train.npy')\n",
    "print(X_train.shape)\n",
    "y_train = np.load(dataset_dir+'y_train.npy')\n",
    "#print(list(y_train).count(0),list(y_train).count(1),list(y_train).count(2),list(y_train).count(3),list(y_train).count(4),list(y_train).count(5),list(y_train).count(6),list(y_train).count(7),list(y_train).count(8),list(y_train).count(9))\n",
    "\n",
    "    # Load testing data\n",
    "X_test = np.load(dataset_dir+'X_test.npy')\n",
    "y_test = np.load(dataset_dir+'y_test.npy')\n",
    "\n",
    "\n",
    "    # Load testing data\n",
    "X_valid = np.load(dataset_dir+'X_valid.npy')\n",
    "y_valid = np.load(dataset_dir+'y_valid.npy')\n",
    "\n",
    "X=np.concatenate((X_train, X_test,X_valid), axis=0)\n",
    "y=np.concatenate((y_train, y_test,y_valid), axis=0)\n",
    "\n",
    "  \n",
    "#X,y=balance_dataset(X,y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X[:5])\n",
    "\n",
    "\n",
    "\n",
    "Split_Number=\"6\"\n",
    "  \n",
    "  \n",
    "X_5,X_open,y_5,y_open=Filter_close_set(Split_Number,data)\n",
    "\n",
    "\n",
    "  \n",
    "X_train_5, X_valid_5, y_train_5, y_valid_5 = train_test_split(X_5, y_5, test_size=0.3, shuffle=False)\n",
    "X_valid_5, X_test_5, y_valid_5, y_test_5 = train_test_split(X_valid_5, y_valid_5, test_size=0.4, shuffle=False)\n",
    "\n",
    "X_train_5,y_train_5=shuffle(X_train_5, y_train_5)\n",
    "X_valid_5,y_valid_5=shuffle(X_valid_5, y_valid_5)\n",
    "X_test_5,y_test_5=shuffle(X_test_5, y_test_5)\n",
    "\n",
    "\n",
    "X_train_5=np.array(X_train_5)\n",
    "X_valid_5=np.array(X_valid_5)\n",
    "X_test_5=np.array(X_test_5)\n",
    "y_train_5=np.array(y_train_5)\n",
    "y_valid_5=np.array(y_valid_5)\n",
    "y_test_5=np.array(y_test_5)\n",
    "X_open=np.array(X_open)\n",
    "y_open=np.array(y_open)\n",
    "\n",
    "\n",
    "#print(y_test)\n",
    "#print(\"##############\")\n",
    "#print(y_test_5)\n",
    "\n",
    "print (\"Data dimensions:\")\n",
    "print (\"X: Training data's shape : \", X_train_5.shape)\n",
    "print (\"X: Validating data's shape : \", X_valid_5.shape)\n",
    "print (\"X: Testing data's shape : \", X_test_5.shape)\n",
    "print (\"X: open data's shape : \", X_open.shape)\n",
    "    \n",
    "np.save('X_train_5.npy',X_train_5)\n",
    "np.save('X_valid_5.npy',X_valid_5)\n",
    "np.save('X_test_5.npy',X_test_5)\n",
    "np.save('y_valid_5.npy',y_valid_5)\n",
    "np.save('y_train_5.npy',y_train_5)\n",
    "np.save('y_test_5.npy',y_test_5)\n",
    "np.save('X_open.npy',X_open)\n",
    "np.save('y_open.npy',y_open)\n",
    "\n",
    "#print(y_train_5[:50])\n",
    "#print()\n",
    "#print(y_valid_5[:50])\n",
    "#print()\n",
    "#print(y_test_5[:50])\n",
    "#print()\n",
    "#print(y_open[:50])\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f24e6975",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T19:26:29.473152Z",
     "iopub.status.busy": "2022-06-21T19:26:29.472871Z",
     "iopub.status.idle": "2022-06-21T19:26:29.512258Z",
     "shell.execute_reply": "2022-06-21T19:26:29.511213Z"
    },
    "papermill": {
     "duration": 0.048781,
     "end_time": "2022-06-21T19:26:29.514129",
     "exception": false,
     "start_time": "2022-06-21T19:26:29.465348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize(X,epsilon=1e-8):\n",
    "\t\t\n",
    "    Raws=X.shape[0]\n",
    "    Columns=X.shape[1]\n",
    "    for i in range(Raws):\n",
    "        Tot=0\n",
    "        for j in X[i]:\n",
    "            Tot+=j\n",
    "            mean=Tot/Columns\n",
    "\n",
    "        Dis=0\n",
    "        for j in X[i]:\n",
    "            Dis+=(j-mean)**2\n",
    "            Var=Dis/Columns\n",
    "\n",
    "        X[i]=(X[i]-mean)/(np.sqrt(Var)+epsilon)\n",
    "                                        \n",
    "    return X\n",
    "\n",
    "# # Load data for non-defended dataset for CW setting\n",
    "def LoadDataIot():\n",
    "\n",
    "    dataset_dir = './'\n",
    "\n",
    "    # X represents a sequence of traffic directions\n",
    "    # y represents a sequence of corresponding label (website's label)\n",
    "\n",
    "    # Load training data\n",
    "    \n",
    "    X_train = load(dataset_dir+'X_train_5.npy')\n",
    "    y_train = load(dataset_dir+'y_train_5.npy')\n",
    "\n",
    "    \n",
    "    # Load validation data\n",
    "    X_valid = load(dataset_dir+'X_valid_5.npy')\n",
    "    y_valid = load(dataset_dir+'y_valid_5.npy')\n",
    "\n",
    "    # Load testing data\n",
    "    X_test = load(dataset_dir+'X_test_5.npy')\n",
    "    y_test = load(dataset_dir+'y_test_5.npy')\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    \n",
    "    print (\"Data dimensions:\")\n",
    "    print (\"X: Training data's shape : \", X_train.shape)\n",
    "    print (\"y: Training data's shape : \", y_train.shape)\n",
    "    print (\"X: Validation data's shape : \", X_valid.shape)\n",
    "    print (\"y: Validation data's shape : \", y_valid.shape)\n",
    "    print (\"X: Testing data's shape : \", X_test.shape)\n",
    "    print (\"y: Testing data's shape : \", y_test.shape)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "\n",
    "class DFNet:\n",
    "    @staticmethod\n",
    "    def build(input_shape, nb_classes=98, trainable=True):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        # Feature Extractor\n",
    "        model.add(Conv1D(128, kernel_size=7, activation='tanh', input_shape=input_shape, use_bias=False, trainable=trainable))\n",
    "        model.add(BatchNormalization(trainable=trainable))\n",
    "        model.add(MaxPooling1D(1, trainable=trainable))\n",
    "        model.add(Dropout(rate=0.1, trainable=trainable))\n",
    "\n",
    "        model.add(Conv1D(128, kernel_size=19, activation='relu',  trainable=trainable))\n",
    "        model.add(BatchNormalization(trainable=trainable))\n",
    "        model.add(MaxPooling1D(1, trainable=trainable))\n",
    "        model.add(Dropout(rate=0.3, trainable=trainable))\n",
    "\n",
    "        model.add(Conv1D(64, kernel_size=13, activation='relu',  trainable=trainable))\n",
    "        model.add(BatchNormalization(trainable=trainable))\n",
    "        model.add(MaxPooling1D(1, trainable=trainable))\n",
    "        model.add(Dropout(rate=0.1, trainable=trainable))\n",
    "\n",
    "        model.add(Conv1D(256, kernel_size=23, activation='relu',  trainable=trainable))\n",
    "        model.add(BatchNormalization(trainable=trainable))\n",
    "        model.add(MaxPooling1D(1, trainable=trainable))\n",
    "        model.add(GlobalAveragePooling1D(trainable=trainable))\n",
    "\n",
    "        model.add(Dense(180, activation='elu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(150, activation='elu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(nb_classes,name='phenultimate_layer'))\n",
    "        model.add(Activation('softmax', name='softmax', trainable=trainable))\n",
    "\n",
    "        return model\n",
    "        \n",
    "class DFNet_Dropout:\n",
    "    @staticmethod\n",
    "    def build(input_shape, nb_classes=98, trainable=True):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        # Feature Extractor\n",
    "        model.add(Conv1D(128, kernel_size=7, activation='tanh', input_shape=input_shape, use_bias=False, trainable=trainable))\n",
    "        model.add(BatchNormalization(trainable=trainable))\n",
    "        model.add(MaxPooling1D(1, trainable=trainable))\n",
    "        model.add(Dropout(rate=0.3, trainable=trainable))\n",
    "\n",
    "        model.add(Conv1D(128, kernel_size=19, activation='relu',  trainable=trainable))\n",
    "        model.add(BatchNormalization(trainable=trainable))\n",
    "        model.add(MaxPooling1D(1, trainable=trainable))\n",
    "        model.add(Dropout(rate=0.5, trainable=trainable))\n",
    "\n",
    "        model.add(Conv1D(64, kernel_size=13, activation='relu',  trainable=trainable))\n",
    "        model.add(BatchNormalization(trainable=trainable))\n",
    "        model.add(MaxPooling1D(1, trainable=trainable))\n",
    "        model.add(Dropout(rate=0.3, trainable=trainable))\n",
    "\n",
    "        model.add(Conv1D(256, kernel_size=23, activation='relu',  trainable=trainable))\n",
    "        model.add(BatchNormalization(trainable=trainable))\n",
    "        model.add(MaxPooling1D(1, trainable=trainable))\n",
    "        model.add(GlobalAveragePooling1D(trainable=trainable))\n",
    "\n",
    "        model.add(Dense(180, activation='elu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(rate=0.5, trainable=trainable))\n",
    "        model.add(Dense(150, activation='elu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(nb_classes,name='phenultimate_layer'))\n",
    "        model.add(Activation('softmax', name='softmax', trainable=trainable))\n",
    "\n",
    "        return model\n",
    "        \n",
    "\n",
    "class DFNet_Add_Layer:\n",
    "    @staticmethod\n",
    "    def build(input_shape, nb_classes=98, trainable=True):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        # Feature Extractor\n",
    "        model.add(Conv1D(128, kernel_size=7, activation='tanh', input_shape=input_shape, use_bias=False, trainable=trainable))\n",
    "        model.add(BatchNormalization(trainable=trainable))\n",
    "        model.add(MaxPooling1D(1, trainable=trainable))\n",
    "        model.add(Dropout(rate=0.3, trainable=trainable))\n",
    "\n",
    "        model.add(Conv1D(128, kernel_size=19, activation='relu',  trainable=trainable))\n",
    "        model.add(BatchNormalization(trainable=trainable))\n",
    "        model.add(MaxPooling1D(1, trainable=trainable))\n",
    "        model.add(Dropout(rate=0.5, trainable=trainable))\n",
    "\n",
    "        model.add(Conv1D(64, kernel_size=13, activation='relu',  trainable=trainable))\n",
    "        model.add(BatchNormalization(trainable=trainable))\n",
    "        model.add(MaxPooling1D(1, trainable=trainable))\n",
    "        model.add(Dropout(rate=0.3, trainable=trainable))\n",
    "        \n",
    "        \n",
    "        model.add(Conv1D(128, kernel_size=17, activation='relu',  trainable=trainable))\n",
    "        model.add(BatchNormalization(trainable=trainable))\n",
    "        model.add(MaxPooling1D(1, trainable=trainable))\n",
    "        model.add(Dropout(rate=0.5, trainable=trainable))\n",
    "\n",
    "\n",
    "        model.add(Conv1D(256, kernel_size=23, activation='relu',  trainable=trainable))\n",
    "        model.add(BatchNormalization(trainable=trainable))\n",
    "        model.add(MaxPooling1D(1, trainable=trainable))\n",
    "        model.add(GlobalAveragePooling1D(trainable=trainable))\n",
    "\n",
    "        model.add(Dense(180, activation='elu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(rate=0.5, trainable=trainable))\n",
    "        model.add(Dense(150, activation='elu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(nb_classes,name='phenultimate_layer'))\n",
    "        model.add(Activation('softmax', name='softmax', trainable=trainable))\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1ca18",
   "metadata": {
    "papermill": {
     "duration": 0.005782,
     "end_time": "2022-06-21T19:26:29.526330",
     "exception": false,
     "start_time": "2022-06-21T19:26:29.520548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d44a8a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T19:26:29.539489Z",
     "iopub.status.busy": "2022-06-21T19:26:29.539221Z",
     "iopub.status.idle": "2022-06-21T20:02:27.730945Z",
     "shell.execute_reply": "2022-06-21T20:02:27.729128Z"
    },
    "papermill": {
     "duration": 2158.201415,
     "end_time": "2022-06-21T20:02:27.733556",
     "exception": false,
     "start_time": "2022-06-21T19:26:29.532141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating DF model for closed-world scenario on non-defended dataset\n",
      "Number of Epoch:  80\n",
      "Loading and preparing data for training, and evaluating the model\n",
      "Data dimensions:\n",
      "X: Training data's shape :  (41962, 475)\n",
      "y: Training data's shape :  (41962,)\n",
      "X: Validation data's shape :  (10790, 475)\n",
      "y: Validation data's shape :  (10790,)\n",
      "X: Testing data's shape :  (7194, 475)\n",
      "y: Testing data's shape :  (7194,)\n",
      "41962 train samples\n",
      "10790 validation samples\n",
      "7194 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 19:26:29.897426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:26:30.012771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:26:30.013990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:26:30.017822: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-21 19:26:30.018370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:26:30.020002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:26:30.021185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:26:32.285450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:26:32.286446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:26:32.287232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:26:32.287867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 19:26:33.054475: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 19:26:35.849180: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 - 36s - loss: 1.6040 - accuracy: 0.4785 - val_loss: 1.3733 - val_accuracy: 0.5650\n",
      "Epoch 2/10\n",
      "600/600 - 25s - loss: 1.0574 - accuracy: 0.6464 - val_loss: 0.8990 - val_accuracy: 0.6945\n",
      "Epoch 3/10\n",
      "600/600 - 25s - loss: 0.7954 - accuracy: 0.7398 - val_loss: 0.7444 - val_accuracy: 0.7459\n",
      "Epoch 4/10\n",
      "600/600 - 25s - loss: 0.6523 - accuracy: 0.7915 - val_loss: 0.4372 - val_accuracy: 0.8618\n",
      "Epoch 5/10\n",
      "600/600 - 25s - loss: 0.5807 - accuracy: 0.8138 - val_loss: 0.4669 - val_accuracy: 0.8543\n",
      "Epoch 6/10\n",
      "600/600 - 26s - loss: 0.5204 - accuracy: 0.8330 - val_loss: 0.3484 - val_accuracy: 0.8992\n",
      "Epoch 7/10\n",
      "600/600 - 25s - loss: 0.4866 - accuracy: 0.8453 - val_loss: 0.3264 - val_accuracy: 0.9049\n",
      "Epoch 8/10\n",
      "600/600 - 25s - loss: 0.4599 - accuracy: 0.8530 - val_loss: 0.2936 - val_accuracy: 0.9187\n",
      "Epoch 9/10\n",
      "600/600 - 25s - loss: 0.4493 - accuracy: 0.8587 - val_loss: 0.2832 - val_accuracy: 0.9222\n",
      "Epoch 10/10\n",
      "600/600 - 25s - loss: 0.4396 - accuracy: 0.8605 - val_loss: 0.2752 - val_accuracy: 0.9233\n",
      "Epoch 1/10\n",
      "600/600 - 25s - loss: 0.4642 - accuracy: 0.8512 - val_loss: 0.3487 - val_accuracy: 0.9003\n",
      "Epoch 2/10\n",
      "600/600 - 25s - loss: 0.4334 - accuracy: 0.8625 - val_loss: 0.2759 - val_accuracy: 0.9259\n",
      "Epoch 3/10\n",
      "600/600 - 27s - loss: 0.3940 - accuracy: 0.8752 - val_loss: 0.2427 - val_accuracy: 0.9367\n",
      "Epoch 4/10\n",
      "600/600 - 25s - loss: 0.3715 - accuracy: 0.8832 - val_loss: 0.2336 - val_accuracy: 0.9385\n",
      "Epoch 5/10\n",
      "600/600 - 25s - loss: 0.3544 - accuracy: 0.8901 - val_loss: 0.2305 - val_accuracy: 0.9361\n",
      "Epoch 6/10\n",
      "600/600 - 25s - loss: 0.3368 - accuracy: 0.8935 - val_loss: 0.2110 - val_accuracy: 0.9484\n",
      "Epoch 7/10\n",
      "600/600 - 25s - loss: 0.3315 - accuracy: 0.8959 - val_loss: 0.2014 - val_accuracy: 0.9488\n",
      "Epoch 8/10\n",
      "600/600 - 25s - loss: 0.3193 - accuracy: 0.8976 - val_loss: 0.1947 - val_accuracy: 0.9488\n",
      "Epoch 9/10\n",
      "600/600 - 25s - loss: 0.3200 - accuracy: 0.8996 - val_loss: 0.1894 - val_accuracy: 0.9500\n",
      "Epoch 10/10\n",
      "600/600 - 25s - loss: 0.3105 - accuracy: 0.9049 - val_loss: 0.1917 - val_accuracy: 0.9501\n",
      "Epoch 1/10\n",
      "600/600 - 26s - loss: 0.3277 - accuracy: 0.8983 - val_loss: 0.2091 - val_accuracy: 0.9464\n",
      "Epoch 2/10\n",
      "600/600 - 25s - loss: 0.3215 - accuracy: 0.8984 - val_loss: 0.1967 - val_accuracy: 0.9494\n",
      "Epoch 3/10\n",
      "600/600 - 25s - loss: 0.3022 - accuracy: 0.9042 - val_loss: 0.1973 - val_accuracy: 0.9523\n",
      "Epoch 4/10\n",
      "600/600 - 25s - loss: 0.2929 - accuracy: 0.9096 - val_loss: 0.1730 - val_accuracy: 0.9532\n",
      "Epoch 5/10\n",
      "600/600 - 25s - loss: 0.2841 - accuracy: 0.9108 - val_loss: 0.1758 - val_accuracy: 0.9563\n",
      "Epoch 6/10\n",
      "600/600 - 25s - loss: 0.2749 - accuracy: 0.9139 - val_loss: 0.1831 - val_accuracy: 0.9562\n",
      "Epoch 7/10\n",
      "600/600 - 25s - loss: 0.2717 - accuracy: 0.9143 - val_loss: 0.1707 - val_accuracy: 0.9578\n",
      "Epoch 8/10\n",
      "600/600 - 25s - loss: 0.2724 - accuracy: 0.9147 - val_loss: 0.1709 - val_accuracy: 0.9587\n",
      "Epoch 9/10\n",
      "600/600 - 25s - loss: 0.2643 - accuracy: 0.9180 - val_loss: 0.1673 - val_accuracy: 0.9589\n",
      "Epoch 10/10\n",
      "600/600 - 25s - loss: 0.2680 - accuracy: 0.9167 - val_loss: 0.1588 - val_accuracy: 0.9599\n",
      "Epoch 1/10\n",
      "600/600 - 26s - loss: 0.2711 - accuracy: 0.9143 - val_loss: 0.1657 - val_accuracy: 0.9574\n",
      "Epoch 2/10\n",
      "600/600 - 25s - loss: 0.2693 - accuracy: 0.9154 - val_loss: 0.1662 - val_accuracy: 0.9587\n",
      "Epoch 3/10\n",
      "600/600 - 25s - loss: 0.2593 - accuracy: 0.9181 - val_loss: 0.1696 - val_accuracy: 0.9592\n",
      "Epoch 4/10\n",
      "600/600 - 25s - loss: 0.2546 - accuracy: 0.9201 - val_loss: 0.1531 - val_accuracy: 0.9602\n",
      "Epoch 5/10\n",
      "600/600 - 27s - loss: 0.2470 - accuracy: 0.9224 - val_loss: 0.1510 - val_accuracy: 0.9628\n",
      "Epoch 6/10\n",
      "600/600 - 25s - loss: 0.2443 - accuracy: 0.9234 - val_loss: 0.1579 - val_accuracy: 0.9602\n",
      "Epoch 7/10\n",
      "600/600 - 25s - loss: 0.2441 - accuracy: 0.9235 - val_loss: 0.1499 - val_accuracy: 0.9605\n",
      "Epoch 8/10\n",
      "600/600 - 25s - loss: 0.2415 - accuracy: 0.9248 - val_loss: 0.1513 - val_accuracy: 0.9626\n",
      "Epoch 9/10\n",
      "600/600 - 25s - loss: 0.2352 - accuracy: 0.9263 - val_loss: 0.1527 - val_accuracy: 0.9630\n",
      "Epoch 10/10\n",
      "600/600 - 25s - loss: 0.2367 - accuracy: 0.9243 - val_loss: 0.1460 - val_accuracy: 0.9627\n",
      "Epoch 1/10\n",
      "600/600 - 25s - loss: 0.2455 - accuracy: 0.9237 - val_loss: 0.1536 - val_accuracy: 0.9595\n",
      "Epoch 2/10\n",
      "600/600 - 25s - loss: 0.2427 - accuracy: 0.9246 - val_loss: 0.1581 - val_accuracy: 0.9601\n",
      "Epoch 3/10\n",
      "600/600 - 25s - loss: 0.2384 - accuracy: 0.9253 - val_loss: 0.1505 - val_accuracy: 0.9652\n",
      "Epoch 4/10\n",
      "600/600 - 27s - loss: 0.2320 - accuracy: 0.9272 - val_loss: 0.1529 - val_accuracy: 0.9637\n",
      "Epoch 5/10\n",
      "600/600 - 25s - loss: 0.2280 - accuracy: 0.9289 - val_loss: 0.1402 - val_accuracy: 0.9645\n",
      "Epoch 6/10\n",
      "600/600 - 25s - loss: 0.2247 - accuracy: 0.9302 - val_loss: 0.1417 - val_accuracy: 0.9645\n",
      "Epoch 7/10\n",
      "600/600 - 25s - loss: 0.2212 - accuracy: 0.9317 - val_loss: 0.1445 - val_accuracy: 0.9645\n",
      "Epoch 8/10\n",
      "600/600 - 25s - loss: 0.2234 - accuracy: 0.9304 - val_loss: 0.1416 - val_accuracy: 0.9653\n",
      "Epoch 9/10\n",
      "600/600 - 25s - loss: 0.2220 - accuracy: 0.9319 - val_loss: 0.1359 - val_accuracy: 0.9659\n",
      "Epoch 10/10\n",
      "600/600 - 25s - loss: 0.2144 - accuracy: 0.9334 - val_loss: 0.1393 - val_accuracy: 0.9657\n",
      "Epoch 1/10\n",
      "600/600 - 25s - loss: 0.2279 - accuracy: 0.9282 - val_loss: 0.1437 - val_accuracy: 0.9657\n",
      "Epoch 2/10\n",
      "600/600 - 25s - loss: 0.2213 - accuracy: 0.9321 - val_loss: 0.1531 - val_accuracy: 0.9592\n",
      "Epoch 3/10\n",
      "600/600 - 25s - loss: 0.2210 - accuracy: 0.9295 - val_loss: 0.1508 - val_accuracy: 0.9631\n",
      "Epoch 4/10\n",
      "600/600 - 27s - loss: 0.2181 - accuracy: 0.9307 - val_loss: 0.1383 - val_accuracy: 0.9670\n",
      "Epoch 5/10\n",
      "600/600 - 25s - loss: 0.2131 - accuracy: 0.9334 - val_loss: 0.1396 - val_accuracy: 0.9660\n",
      "Epoch 6/10\n",
      "600/600 - 25s - loss: 0.2105 - accuracy: 0.9342 - val_loss: 0.1346 - val_accuracy: 0.9664\n",
      "Epoch 7/10\n",
      "600/600 - 25s - loss: 0.2060 - accuracy: 0.9349 - val_loss: 0.1382 - val_accuracy: 0.9665\n",
      "Epoch 8/10\n",
      "600/600 - 25s - loss: 0.2069 - accuracy: 0.9348 - val_loss: 0.1277 - val_accuracy: 0.9678\n",
      "Epoch 9/10\n",
      "600/600 - 25s - loss: 0.2062 - accuracy: 0.9342 - val_loss: 0.1268 - val_accuracy: 0.9677\n",
      "Epoch 10/10\n",
      "600/600 - 25s - loss: 0.2024 - accuracy: 0.9366 - val_loss: 0.1355 - val_accuracy: 0.9674\n",
      "Epoch 1/10\n",
      "600/600 - 25s - loss: 0.2133 - accuracy: 0.9336 - val_loss: 0.1408 - val_accuracy: 0.9650\n",
      "Epoch 2/10\n",
      "600/600 - 25s - loss: 0.2076 - accuracy: 0.9345 - val_loss: 0.1323 - val_accuracy: 0.9688\n",
      "Epoch 3/10\n",
      "600/600 - 25s - loss: 0.2006 - accuracy: 0.9373 - val_loss: 0.1280 - val_accuracy: 0.9672\n",
      "Epoch 4/10\n",
      "600/600 - 27s - loss: 0.2071 - accuracy: 0.9358 - val_loss: 0.1252 - val_accuracy: 0.9682\n",
      "Epoch 5/10\n",
      "600/600 - 25s - loss: 0.2014 - accuracy: 0.9371 - val_loss: 0.1287 - val_accuracy: 0.9683\n",
      "Epoch 6/10\n",
      "600/600 - 25s - loss: 0.1958 - accuracy: 0.9393 - val_loss: 0.1280 - val_accuracy: 0.9689\n",
      "Epoch 7/10\n",
      "600/600 - 25s - loss: 0.1997 - accuracy: 0.9366 - val_loss: 0.1238 - val_accuracy: 0.9696\n",
      "Epoch 8/10\n",
      "600/600 - 25s - loss: 0.1978 - accuracy: 0.9370 - val_loss: 0.1272 - val_accuracy: 0.9691\n",
      "Epoch 9/10\n",
      "600/600 - 25s - loss: 0.1963 - accuracy: 0.9376 - val_loss: 0.1204 - val_accuracy: 0.9694\n",
      "Epoch 10/10\n",
      "600/600 - 25s - loss: 0.1997 - accuracy: 0.9381 - val_loss: 0.1209 - val_accuracy: 0.9696\n",
      "Epoch 1/10\n",
      "600/600 - 25s - loss: 0.1904 - accuracy: 0.9403 - val_loss: 0.1198 - val_accuracy: 0.9698\n",
      "Epoch 2/10\n",
      "600/600 - 25s - loss: 0.1973 - accuracy: 0.9372 - val_loss: 0.1248 - val_accuracy: 0.9694\n",
      "Epoch 3/10\n",
      "600/600 - 27s - loss: 0.1985 - accuracy: 0.9381 - val_loss: 0.1277 - val_accuracy: 0.9685\n",
      "Epoch 4/10\n",
      "600/600 - 25s - loss: 0.1922 - accuracy: 0.9386 - val_loss: 0.1279 - val_accuracy: 0.9702\n",
      "Epoch 5/10\n",
      "600/600 - 25s - loss: 0.1893 - accuracy: 0.9407 - val_loss: 0.1282 - val_accuracy: 0.9692\n",
      "Epoch 6/10\n",
      "600/600 - 25s - loss: 0.1921 - accuracy: 0.9401 - val_loss: 0.1234 - val_accuracy: 0.9694\n",
      "Epoch 7/10\n",
      "600/600 - 25s - loss: 0.1854 - accuracy: 0.9414 - val_loss: 0.1247 - val_accuracy: 0.9703\n",
      "Epoch 8/10\n",
      "600/600 - 25s - loss: 0.1872 - accuracy: 0.9396 - val_loss: 0.1224 - val_accuracy: 0.9699\n",
      "Epoch 9/10\n",
      "600/600 - 25s - loss: 0.1861 - accuracy: 0.9411 - val_loss: 0.1210 - val_accuracy: 0.9705\n",
      "Epoch 10/10\n",
      "600/600 - 25s - loss: 0.1796 - accuracy: 0.9433 - val_loss: 0.1230 - val_accuracy: 0.9702\n",
      "(7194, 40)\n",
      "(7194,)\n",
      "Testing closed accuracy_without_norm: 0.9703919933277732\n",
      "(80,)\n",
      "[0.08       0.07804226 0.07236068 0.06351141 0.05236068 0.04\n",
      " 0.02763932 0.01648859 0.00763932 0.00195774]\n",
      "(80,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "#from Model_DF import DFNet\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "snapshot_number=8\n",
    "\n",
    "#MyAdamW = extend_with_decoupled_weight_decay(Adam)\n",
    "# Create a MyAdamW object\n",
    "#OPTIMIZER = MyAdamW(weight_decay=0.001, learning_rate=0.0001,beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# ...\n",
    "learning_rate_data=[]\n",
    "loss_data=[]\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "   initial_lrate = 0.08\n",
    "   \n",
    "   if epoch<2 and i>0:\n",
    "     lrate=0.1\n",
    "   \n",
    "   else:\n",
    "     lrate = initial_lrate * (0.5+0.5*np.cos(epoch*math.pi/(NB_EPOCH//snapshot_number)))\n",
    "\n",
    "   learning_rate_data.append(lrate)\n",
    "   optimizer = model.optimizer\n",
    "   optimizer.lr = lrate\n",
    "   return lrate\n",
    "   \n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "description = \"Training and evaluating DF model for closed-world scenario on non-defended dataset\"\n",
    "\n",
    "print (description)\n",
    "# Training the DF model\n",
    "NB_EPOCH = 80   # Number of training epoch\n",
    "print (\"Number of Epoch: \", NB_EPOCH)\n",
    "BATCH_SIZE = 70 # Batch size\n",
    "VERBOSE = 2 # Output display mode\n",
    "LENGTH = 475 # Packet sequence length\n",
    "OPTIMIZER = Adamax(learning_rate=0.05, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.003) # Optimizer\n",
    "\n",
    "NB_CLASSES = 40 # number of outputs = number of classes\n",
    "INPUT_SHAPE = (LENGTH,1)\n",
    "\n",
    "\n",
    "# Data: shuffled and split between train and test sets\n",
    "\n",
    "print (\"Loading and preparing data for training, and evaluating the model\")\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = LoadDataIot()\n",
    "# Please refer to the dataset format in readme\n",
    "\n",
    "# Convert data as float32 type\n",
    "X_train = X_train.astype('float32')\n",
    "X_valid = X_valid.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_valid = y_valid.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "# we need a [Length x 1] x n shape as input to the DF CNN (Tensorflow)\n",
    "X_train = X_train[:, :,np.newaxis]\n",
    "X_valid = X_valid[:, :,np.newaxis]\n",
    "X_test = X_test[:, :,np.newaxis]\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_valid.shape[0], 'validation samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to categorical classes matrices\n",
    "y_train = to_categorical(y_train, NB_CLASSES)\n",
    "y_valid = to_categorical(y_valid, NB_CLASSES)\n",
    "#y_test = to_categorical(y_test, NB_CLASSES)\n",
    "# Building and training model\n",
    "# print (\"Building and training DF model\")\n",
    "\n",
    "model = DFNet_Add_Layer.build(input_shape=INPUT_SHAPE, nb_classes=NB_CLASSES)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "print (\"Model compiled\")\n",
    "\n",
    "filepath = 'IoT.hdf5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint,lrate]\n",
    "\n",
    "Predictions=[]\n",
    "# Start training\n",
    "for i in range(snapshot_number):\n",
    "  # callbacks_list.append(EarlyStopping(monitor='val_acc', mode='max', patience=6))\n",
    "  #model.summary()\n",
    "  \n",
    "  history = model.fit(X_train, y_train,batch_size=BATCH_SIZE, epochs=NB_EPOCH//snapshot_number,verbose=VERBOSE, validation_data=(X_valid, y_valid), callbacks=callbacks_list)\n",
    "  losss=history.history['val_loss']\n",
    "  #print(losss.shape)\n",
    "  loss_data+=losss\n",
    "  \n",
    "  Predictions.append(model.predict(X_test))\n",
    "  \n",
    "  model2 = Model(model.input, model.layers[-2].output)\n",
    "  model2.save('snapshot_'+str(i)+'.hdf5')\n",
    "\n",
    "\n",
    "#model=load_model('AWF.hdf5')\n",
    "\n",
    "# Start evaluating model with testing data\n",
    "Predictions=np.array(Predictions)\n",
    "Avg_prediction=np.average(Predictions,axis=0)\n",
    "print(Avg_prediction.shape)\n",
    "\n",
    "\n",
    "Avg_prediction=np.argmax(Avg_prediction,axis=1)\n",
    "print(Avg_prediction.shape)\n",
    "        \n",
    "#print(\"############## Training is Done Successfully ###################\")\n",
    "#model.save('DC_without_norm.hdf5')\n",
    "\n",
    "#model=load_model('AWF.hdf5')\n",
    "\n",
    "# Start evaluating model with testing data\n",
    "score_test = accuracy_score(Avg_prediction, y_test)\n",
    "print(\"Testing closed accuracy_without_norm:\", score_test)\n",
    "\n",
    "lr=history.history['lr']\n",
    "learning_rate_data=np.array(learning_rate_data)\n",
    "print(learning_rate_data.shape)\n",
    "print(learning_rate_data[:10])\n",
    "\n",
    "loss_data=np.array(loss_data)\n",
    "print(loss_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca206374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T20:02:27.766822Z",
     "iopub.status.busy": "2022-06-21T20:02:27.766530Z",
     "iopub.status.idle": "2022-06-21T20:03:34.753756Z",
     "shell.execute_reply": "2022-06-21T20:03:34.752418Z"
    },
    "papermill": {
     "duration": 67.006248,
     "end_time": "2022-06-21T20:03:34.756215",
     "exception": false,
     "start_time": "2022-06-21T20:02:27.749967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions:\n",
      "X: Training data's shape :  (41962, 475)\n",
      "y: Training data's shape :  (41962,)\n",
      "X: Validation data's shape :  (10790, 475)\n",
      "y: Validation data's shape :  (10790,)\n",
      "X: Testing data's shape :  (7194, 475)\n",
      "y: Testing data's shape :  (7194,)\n",
      "41962 train samples\n",
      "10790 validation samples\n",
      "7194 test samples\n",
      "Shape:  (41962, 40)\n",
      "Shape:  (10790, 40)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_valid, y_valid, X_test, y_test = LoadDataIot()\n",
    "# Please refer to the dataset format in readme\n",
    "\n",
    "# Convert data as float32 type\n",
    "X_train = X_train.astype('float32')\n",
    "X_valid = X_valid.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('int16')\n",
    "y_valid = y_valid.astype('int16')\n",
    "y_test = y_test.astype('int16')\n",
    "# we need a [Length x 1] x n shape as input to the DF CNN (Tensorflow)\n",
    "X_train = X_train[:, :,np.newaxis]\n",
    "X_valid = X_valid[:, :,np.newaxis]\n",
    "X_test = X_test[:, :,np.newaxis]\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_valid.shape[0], 'validation samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "#y_test_=np.argmax(y_test_, axis=1)\n",
    "txt_O = \"Mean_{Class1:.0f}\"\n",
    "Means={}\n",
    "for i in range(NB_CLASSES):\n",
    "  Means[txt_O.format(Class1=i)]=np.array([0]*NB_CLASSES)\n",
    "  \n",
    "dataset_dir='./'\n",
    "\n",
    "tflite_model_predictions=[]\n",
    "for i in range(snapshot_number):\n",
    "  model=load_model(dataset_dir+'snapshot_'+str(i)+'.hdf5')\n",
    "  tflite_model_predictions.append(model.predict(X_train))\n",
    "  \n",
    "tflite_model_predictions=np.array(tflite_model_predictions)\n",
    "tflite_model_predictions=np.average(tflite_model_predictions,axis=0)\n",
    "\n",
    "\n",
    "print('Shape: ',tflite_model_predictions.shape)\n",
    "count=[0]*NB_CLASSES\n",
    "\n",
    "for i in range(len(tflite_model_predictions)):\n",
    "  k=np.argmax(tflite_model_predictions[i])\n",
    "  if (np.argmax(tflite_model_predictions[i])==y_train[i]):\n",
    "    Means[txt_O.format(Class1=y_train[i])]=Means[txt_O.format(Class1=y_train[i])]+tflite_model_predictions[i]\n",
    "    count[y_train[i]]+=1\n",
    "#print(\"Counts: \",count)\n",
    "\n",
    "Mean_Vectors=[]   \n",
    "for i in range(NB_CLASSES):\n",
    "  Means[txt_O.format(Class1=i)]=Means[txt_O.format(Class1=i)]/count[i]\n",
    "  Mean_Vectors.append(Means[txt_O.format(Class1=i)])\n",
    "\n",
    "Mean_vectors=np.array(Mean_Vectors)\n",
    "np.save('Mean_vectors.npy', Mean_vectors, allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "tflite_model_predictions=[]\n",
    "for i in range(snapshot_number):\n",
    "  model=load_model(dataset_dir+'snapshot_'+str(i)+'.hdf5')\n",
    "  tflite_model_predictions.append(model.predict(X_valid))\n",
    "    \n",
    "tflite_model_predictions=np.array(tflite_model_predictions)\n",
    "tflite_model_predictions=np.average(tflite_model_predictions,axis=0)\n",
    "\n",
    "\n",
    "print('Shape: ',tflite_model_predictions.shape)\n",
    "\n",
    "txt_1 = \"Dist_{Class1:.0f}\"\n",
    "Distances={}\n",
    "for i in range(NB_CLASSES):\n",
    "  Distances[txt_1.format(Class1=i)]=[]\n",
    "  \n",
    "  \n",
    "for i in range(len(tflite_model_predictions)):\n",
    "  if (y_valid[i]==np.argmax(tflite_model_predictions[i])):\n",
    "    dist = np.linalg.norm(Mean_Vectors[y_valid[i]]-tflite_model_predictions[i])\n",
    "    Distances[txt_1.format(Class1=y_valid[i])].append(dist)\n",
    "\n",
    "#print(Distances)    \n",
    "TH=[0]*NB_CLASSES  \n",
    "for j in range(NB_CLASSES):\n",
    "  Distances[txt_1.format(Class1=j)].sort()\n",
    "  Dist=Distances[txt_1.format(Class1=j)]\n",
    "  TH[j]=Dist[int(len(Dist)*0.90)]  \n",
    "\n",
    "\n",
    "\n",
    "Threasholds=np.array(TH)\n",
    "#print(Threasholds)\n",
    "np.save('Threasholds_s.npy',Threasholds)\n",
    "\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cdebbc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T20:03:34.790098Z",
     "iopub.status.busy": "2022-06-21T20:03:34.789820Z",
     "iopub.status.idle": "2022-06-21T20:05:37.370640Z",
     "shell.execute_reply": "2022-06-21T20:05:37.369662Z"
    },
    "papermill": {
     "duration": 122.616937,
     "end_time": "2022-06-21T20:05:37.389069",
     "exception": false,
     "start_time": "2022-06-21T20:03:34.772132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_open shape: (86795, 475)\n",
      "\n",
      "Test accuracy TFLITE model_Closed_set : 0.8696135668612733\n",
      "Test accuracy TFLITE model_Open_set : 0.5885131632006452\n",
      "Precision:  0.12099768074590468\n",
      "Recall:  0.8731659174364073\n",
      "Average F1_Score:  0.21254258362834952\n",
      "\n",
      "Micro\n",
      "Test accuracy TFLITE model_Closed_set : 0.8696135668612733\n",
      "Test accuracy TFLITE model_Open_set : 0.5885131632006452\n",
      "Micro Precision:  0.1486798013166338\n",
      "Micro Recall:  0.8696135668612733\n",
      "Average F1_Score:  0.25394247888469385\n",
      "\n",
      "Macro\n",
      "Test accuracy TFLITE model_Closed_set : 0.8696135668612733\n",
      "Test accuracy TFLITE model_Open_set : 0.5885131632006452\n",
      "Precision: 0.2740197322870187\n",
      "Recall: 0.8695997184602374\n",
      "Average F1_Score:  0.4167251261947587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train_Rep).batch(1).take(100):\n",
    "    # Model has only one input so each data point has one element.\n",
    "    yield [input_value]\n",
    "    \n",
    "def Macro_F1(Matrix):\n",
    "  Precisions=np.zeros(NB_CLASSES)\n",
    "  Recalls=np.zeros(NB_CLASSES)\n",
    "  \n",
    "  epsilon=1e-8\n",
    "  \n",
    "  for k in range(len(Precisions)):\n",
    "    Precisions[k]=Matrix[k][k]/np.sum(Matrix,axis=0)[k]\n",
    "  #print(Precisions)\n",
    "    \n",
    "  Precision=np.average(Precisions)\n",
    "  print(\"Precision:\",Precision)\n",
    "    \n",
    "  for k in range(len(Recalls)):\n",
    "    Recalls[k]=Matrix[k][k]/np.sum(Matrix,axis=1)[k]\n",
    "   \n",
    "  Recall=np.average(Recalls)\n",
    "  print(\"Recall:\",Recall)\n",
    "\n",
    "  F1_Score=2*Precision*Recall/(Precision+Recall+epsilon)\n",
    "  return F1_Score\n",
    "    \n",
    "def Micro_F1(matrix):\n",
    "  epsilon=1e-8\n",
    "  TP=0\n",
    "  FP=0\n",
    "  TN=0\n",
    "  \n",
    "  for k in range(NB_CLASSES):\n",
    "    TP+=matrix[k][k]\n",
    "    FP+=(np.sum(Matrix,axis=0)[k]-matrix[k][k])\n",
    "    TN+=(np.sum(Matrix,axis=1)[k]-matrix[k][k])\n",
    "    \n",
    "  Micro_Prec=TP/(TP+FP)\n",
    "  Micro_Rec=TP/(TP+TN)\n",
    "  print(\"Micro Precision: \", Micro_Prec)\n",
    "  print(\"Micro Recall: \", Micro_Rec)\n",
    "  Micro_F1=2*Micro_Prec*Micro_Rec/(Micro_Rec+Micro_Prec+epsilon)\n",
    "  \n",
    "  return Micro_F1\n",
    "\n",
    "\n",
    "def New_F1_Score(Matrix):\n",
    "  Column_sum=np.sum(Matrix,axis=0)\n",
    "  Raw_sum=np.sum(Matrix,axis=1)\n",
    "  \n",
    "  Precision_Differences=[]\n",
    "  Recall_Differences=[]\n",
    "  for i in range(NB_CLASSES):\n",
    "    Precision_Differences.append(np.abs(2*Matrix[i][i]-Column_sum[i]))\n",
    "    Recall_Differences.append(np.abs(2*Matrix[i][i]-Raw_sum[i]))\n",
    "  \n",
    "  Precision_Differences=np.array(Precision_Differences)\n",
    "  Precision_Differences_Per=Precision_Differences/np.sum(Precision_Differences)\n",
    "  Recall_Differences=np.array(Recall_Differences)\n",
    "  Recall_Differences_Per=Recall_Differences/np.sum(Recall_Differences)\n",
    "  \n",
    "  #print('Precision_Differences_Per',Precision_Differences_Per)\n",
    "  #print('Recall_Differences_Per',Recall_Differences_Per)\n",
    "  \n",
    "  Precisions=np.zeros(NB_CLASSES)\n",
    "  Recalls=np.zeros(NB_CLASSES)\n",
    "  \n",
    "  epsilon=1e-8\n",
    "  \n",
    "  for k in range(len(Precisions)):\n",
    "    Precisions[k]=(Matrix[k][k]/np.sum(Matrix,axis=0)[k])\n",
    "  #print(Precisions)\n",
    "  Precision=np.sum(np.array(Precisions)*Precision_Differences_Per)\n",
    "  \n",
    "  for k in range(len(Recalls)):\n",
    "    Recalls[k]=(Matrix[k][k]/np.sum(Matrix,axis=1)[k])  #*Recall_Differences_Per[k]\n",
    "  Recall=np.sum(np.array(Recalls)*Recall_Differences_Per)\n",
    "  \n",
    "  print('Precision: ',Precision)\n",
    "  print('Recall: ',Recall)\n",
    "    \n",
    "  \n",
    "  F1_Score=2*Precision*Recall/(Precision+Recall+epsilon)\n",
    "  return F1_Score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_open=np.load(dataset_dir+'X_open.npy')\n",
    "X_open=X_open[:,:1500]\n",
    "y_open = np.array([NB_CLASSES]*len(X_open))\n",
    "\n",
    "X_test,y_test=shuffle(X_test, y_test)\n",
    "#X_open,y_open=shuffle(X_open, y_open)\n",
    "\n",
    "#X_open=X_open[:5000]\n",
    "#y_open=y_open[:5000]\n",
    "\n",
    "\n",
    "print(\"X_open shape:\", X_open.shape)\n",
    "#print(X_test[0])\n",
    "\n",
    "\n",
    "# Convert data as float32 type\n",
    "#X_train = X_train.astype('float32')\n",
    "#X_valid = X_valid.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_open = X_open.astype('float32')\n",
    "#y_train = y_train.astype('float32')\n",
    "#y_valid = y_valid.astype('float32')\n",
    "y_test = y_test.astype('int16')\n",
    "#y_open = y_open.astype('int8')\n",
    "# we need a [Length x 1] x n shape as input to the DF CNN (Tensorflow)\n",
    "#X_train = X_train[:, :,np.newaxis]\n",
    "#X_valid = X_valid[:, :,np.newaxis]\n",
    "\n",
    "\n",
    "print()\n",
    "#print(X_train[0])\n",
    "#print(X_valid[0])\n",
    "#print(\"################################\")\n",
    "#print(X_test[0])\n",
    "\n",
    "#model=load_model('IoT_without_softmax_Robust_Addlayer_s10.hdf5')\n",
    "Mean_Vectors=np.load('Mean_vectors.npy')\n",
    "Threasholds=np.load('Threasholds_s.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_open = X_open[:, :,np.newaxis]\n",
    "\n",
    "\n",
    "prediction_classes=[]\n",
    "tflite_model_predictions=[]\n",
    "for i in range(snapshot_number):\n",
    "  model=load_model('snapshot_'+str(i)+'.hdf5')\n",
    "  tflite_model_predictions.append(model.predict(X_test))\n",
    "  \n",
    "tflite_model_predictions=np.array(tflite_model_predictions)\n",
    "tflite_model_predictions=np.average(tflite_model_predictions,axis=0)\n",
    "\n",
    "for i in range(len(tflite_model_predictions)):\n",
    "  \n",
    "    d=np.argmax(tflite_model_predictions[i], axis=0)\n",
    "    if np.linalg.norm(tflite_model_predictions[i]-Mean_Vectors[d])>Threasholds[d]:\n",
    "      prediction_classes.append(NB_CLASSES)\n",
    "      \n",
    "    else:\n",
    "      prediction_classes.append(d)\n",
    "      \n",
    "\n",
    "    \n",
    "\n",
    "prediction_classes_open=[]\n",
    "tflite_model_predictions_open=[]\n",
    "for i in range(snapshot_number):\n",
    "  model=load_model('snapshot_'+str(i)+'.hdf5')\n",
    "  tflite_model_predictions_open.append(model.predict(X_open))\n",
    "  \n",
    "tflite_model_predictions_open=np.array(tflite_model_predictions_open)\n",
    "tflite_model_predictions_open=np.average(tflite_model_predictions_open,axis=0)\n",
    "\n",
    "for i in range(len(tflite_model_predictions_open)):\n",
    "    d=np.argmax(tflite_model_predictions_open[i], axis=0)\n",
    "    if np.linalg.norm(tflite_model_predictions_open[i]-Mean_Vectors[d])>Threasholds[d]:\n",
    "      prediction_classes_open.append(NB_CLASSES)\n",
    "      \n",
    "    else:\n",
    "      prediction_classes_open.append(d)\n",
    "      \n",
    "    \n",
    "    \n",
    "acc_Close = accuracy_score(prediction_classes, y_test)\n",
    "print('Test accuracy TFLITE model_Closed_set :', acc_Close)\n",
    "\n",
    "acc_Open = accuracy_score(prediction_classes_open, y_open)\n",
    "print('Test accuracy TFLITE model_Open_set :', acc_Open)\n",
    "\n",
    "\n",
    "Matrix=[]\n",
    "for i in range(NB_CLASSES+1):\n",
    "  Matrix.append(np.zeros(NB_CLASSES+1))\n",
    "  \n",
    "  \n",
    "for i in range(len(y_test)):\n",
    "  Matrix[y_test[i]][prediction_classes[i]]+=1\n",
    "  \n",
    "for i in range(len(y_open)):\n",
    "  Matrix[y_open[i]][prediction_classes_open[i]]+=1\n",
    "  \n",
    "#print(Matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "F1_Score=New_F1_Score(Matrix)\n",
    "\n",
    "\n",
    "#print(\"Average Precision: \", Precision)\n",
    "#print(\"Average Recall: \", Recall)\n",
    "print(\"Average F1_Score: \", F1_Score)\n",
    "\n",
    "print()\n",
    "print(\"Micro\")\n",
    "print('Test accuracy TFLITE model_Closed_set :', acc_Close)\n",
    "print('Test accuracy TFLITE model_Open_set :', acc_Open)\n",
    "F1_Score=Micro_F1(Matrix)\n",
    "print(\"Average F1_Score: \", F1_Score)\n",
    "\n",
    "print()\n",
    "print(\"Macro\")\n",
    "print('Test accuracy TFLITE model_Closed_set :', acc_Close)\n",
    "print('Test accuracy TFLITE model_Open_set :', acc_Open)\n",
    "F1_Score=Macro_F1(Matrix)\n",
    "print(\"Average F1_Score: \", F1_Score)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37c878b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T20:05:37.422362Z",
     "iopub.status.busy": "2022-06-21T20:05:37.422056Z",
     "iopub.status.idle": "2022-06-21T20:05:37.426987Z",
     "shell.execute_reply": "2022-06-21T20:05:37.426073Z"
    },
    "papermill": {
     "duration": 0.024393,
     "end_time": "2022-06-21T20:05:37.429090",
     "exception": false,
     "start_time": "2022-06-21T20:05:37.404697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "print(\"################################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559fca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T17:48:18.968563Z",
     "iopub.status.busy": "2022-06-21T17:48:18.968128Z",
     "iopub.status.idle": "2022-06-21T17:48:40.358952Z",
     "shell.execute_reply": "2022-06-21T17:48:40.357367Z",
     "shell.execute_reply.started": "2022-06-21T17:48:18.968518Z"
    },
    "papermill": {
     "duration": 0.015434,
     "end_time": "2022-06-21T20:05:37.460084",
     "exception": false,
     "start_time": "2022-06-21T20:05:37.444650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf6be991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T20:05:37.492729Z",
     "iopub.status.busy": "2022-06-21T20:05:37.492444Z",
     "iopub.status.idle": "2022-06-21T20:05:51.707789Z",
     "shell.execute_reply": "2022-06-21T20:05:51.706740Z"
    },
    "papermill": {
     "duration": 14.234681,
     "end_time": "2022-06-21T20:05:51.710236",
     "exception": false,
     "start_time": "2022-06-21T20:05:37.475555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'closed': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], 'open': [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]}\n",
      "(88044, 475)\n",
      "(146741, 475)\n",
      "(146741,)\n",
      "[[ 1.  1.  1. ...  0.  0.  0.]\n",
      " [ 1. -1. -1. ...  0.  0.  0.]\n",
      " [ 1.  1.  1. ...  0.  0.  0.]\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [ 1. -1.  1. ...  0.  0.  0.]]\n",
      "Data dimensions:\n",
      "X: Training data's shape :  (41960, 475)\n",
      "X: Validating data's shape :  (10790, 475)\n",
      "X: Testing data's shape :  (7194, 475)\n",
      "X: open data's shape :  (86797, 475)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def Exchange_classes(c1,c2,y):\n",
    "  for i in range(len(y)):\n",
    "    if y[i]==c1:\n",
    "      y[i]=c2\n",
    "      \n",
    "    elif y[i]==c2:\n",
    "      y[i]=c1\n",
    "\n",
    "\n",
    "def balance_dataset(X,y):\n",
    "  Classes=int(np.max(y))+1\n",
    "  k=len(y)//Classes-1\n",
    "  X_=list(np.zeros(k*Classes))\n",
    "  y_=list(np.zeros(k*Classes))\n",
    "  \n",
    "  \n",
    "  for j in range(Classes):\n",
    "    for i in range(k):\n",
    "      y_[i*Classes+j]=j\n",
    "      Index=list(y).index(j)\n",
    "      X_[i*Classes+j]=X[Index]\n",
    "      y[Index]=1000\n",
    "  \n",
    "  y=np.array(y_)\n",
    "  X=np.array(X_)\n",
    "  return X,y\n",
    "  \n",
    "def Filter_close_set(Split_Number,data):\n",
    "  closed_set=data[Split_Number][\"closed\"]\n",
    "  open_set=data[Split_Number][\"open\"]\n",
    "  X_5=[]\n",
    "  X_open=[]\n",
    "  y_5=[]\n",
    "  y_open=[]\n",
    "  for i in range(len(y)):\n",
    "    if y[i] in closed_set:\n",
    "      y_5.append(closed_set.index(y[i]))\n",
    "      X_5.append(X[i])\n",
    "    else:\n",
    "      y_open.append(40)\n",
    "      X_open.append(X[i])\n",
    "      \n",
    "  return X_5,X_open,y_5,y_open\n",
    "  \n",
    "  \n",
    "f = open('../input/sydney-data/IOT_data_file.json')\n",
    "data = json.load(f)\n",
    "print(data[\"1\"])  \n",
    "\n",
    "\n",
    "dataset_dir = \"../input/sydney-data/dataset/\"\n",
    "X_train = np.load(dataset_dir+'X_train.npy')\n",
    "print(X_train.shape)\n",
    "y_train = np.load(dataset_dir+'y_train.npy')\n",
    "#print(list(y_train).count(0),list(y_train).count(1),list(y_train).count(2),list(y_train).count(3),list(y_train).count(4),list(y_train).count(5),list(y_train).count(6),list(y_train).count(7),list(y_train).count(8),list(y_train).count(9))\n",
    "\n",
    "    # Load testing data\n",
    "X_test = np.load(dataset_dir+'X_test.npy')\n",
    "y_test = np.load(dataset_dir+'y_test.npy')\n",
    "\n",
    "\n",
    "    # Load testing data\n",
    "X_valid = np.load(dataset_dir+'X_valid.npy')\n",
    "y_valid = np.load(dataset_dir+'y_valid.npy')\n",
    "\n",
    "X=np.concatenate((X_train, X_test,X_valid), axis=0)\n",
    "y=np.concatenate((y_train, y_test,y_valid), axis=0)\n",
    "\n",
    "  \n",
    "#X,y=balance_dataset(X,y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X[:5])\n",
    "\n",
    "\n",
    "\n",
    "Split_Number=\"7\"\n",
    "  \n",
    "  \n",
    "X_5,X_open,y_5,y_open=Filter_close_set(Split_Number,data)\n",
    "\n",
    "\n",
    "  \n",
    "X_train_5, X_valid_5, y_train_5, y_valid_5 = train_test_split(X_5, y_5, test_size=0.3, shuffle=False)\n",
    "X_valid_5, X_test_5, y_valid_5, y_test_5 = train_test_split(X_valid_5, y_valid_5, test_size=0.4, shuffle=False)\n",
    "\n",
    "X_train_5,y_train_5=shuffle(X_train_5, y_train_5)\n",
    "X_valid_5,y_valid_5=shuffle(X_valid_5, y_valid_5)\n",
    "X_test_5,y_test_5=shuffle(X_test_5, y_test_5)\n",
    "\n",
    "\n",
    "X_train_5=np.array(X_train_5)\n",
    "X_valid_5=np.array(X_valid_5)\n",
    "X_test_5=np.array(X_test_5)\n",
    "y_train_5=np.array(y_train_5)\n",
    "y_valid_5=np.array(y_valid_5)\n",
    "y_test_5=np.array(y_test_5)\n",
    "X_open=np.array(X_open)\n",
    "y_open=np.array(y_open)\n",
    "\n",
    "\n",
    "#print(y_test)\n",
    "#print(\"##############\")\n",
    "#print(y_test_5)\n",
    "\n",
    "print (\"Data dimensions:\")\n",
    "print (\"X: Training data's shape : \", X_train_5.shape)\n",
    "print (\"X: Validating data's shape : \", X_valid_5.shape)\n",
    "print (\"X: Testing data's shape : \", X_test_5.shape)\n",
    "print (\"X: open data's shape : \", X_open.shape)\n",
    "    \n",
    "np.save('X_train_5.npy',X_train_5)\n",
    "np.save('X_valid_5.npy',X_valid_5)\n",
    "np.save('X_test_5.npy',X_test_5)\n",
    "np.save('y_valid_5.npy',y_valid_5)\n",
    "np.save('y_train_5.npy',y_train_5)\n",
    "np.save('y_test_5.npy',y_test_5)\n",
    "np.save('X_open.npy',X_open)\n",
    "np.save('y_open.npy',y_open)\n",
    "\n",
    "#print(y_train_5[:50])\n",
    "#print()\n",
    "#print(y_valid_5[:50])\n",
    "#print()\n",
    "#print(y_test_5[:50])\n",
    "#print()\n",
    "#print(y_open[:50])\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce49ae82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T20:05:51.745377Z",
     "iopub.status.busy": "2022-06-21T20:05:51.745041Z",
     "iopub.status.idle": "2022-06-21T20:40:54.850885Z",
     "shell.execute_reply": "2022-06-21T20:40:54.849855Z"
    },
    "papermill": {
     "duration": 2103.125881,
     "end_time": "2022-06-21T20:40:54.853219",
     "exception": false,
     "start_time": "2022-06-21T20:05:51.727338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating DF model for closed-world scenario on non-defended dataset\n",
      "Number of Epoch:  80\n",
      "Loading and preparing data for training, and evaluating the model\n",
      "Data dimensions:\n",
      "X: Training data's shape :  (41960, 475)\n",
      "y: Training data's shape :  (41960,)\n",
      "X: Validation data's shape :  (10790, 475)\n",
      "y: Validation data's shape :  (10790,)\n",
      "X: Testing data's shape :  (7194, 475)\n",
      "y: Testing data's shape :  (7194,)\n",
      "41960 train samples\n",
      "10790 validation samples\n",
      "7194 test samples\n",
      "Model compiled\n",
      "Epoch 1/10\n",
      "600/600 - 28s - loss: 1.6055 - accuracy: 0.4944 - val_loss: 1.1106 - val_accuracy: 0.6350\n",
      "Epoch 2/10\n",
      "600/600 - 25s - loss: 1.0641 - accuracy: 0.6539 - val_loss: 0.8302 - val_accuracy: 0.7375\n",
      "Epoch 3/10\n",
      "600/600 - 25s - loss: 0.8622 - accuracy: 0.7192 - val_loss: 0.6340 - val_accuracy: 0.8024\n",
      "Epoch 4/10\n",
      "600/600 - 25s - loss: 0.7554 - accuracy: 0.7565 - val_loss: 0.4960 - val_accuracy: 0.8467\n",
      "Epoch 5/10\n",
      "600/600 - 25s - loss: 0.6606 - accuracy: 0.7920 - val_loss: 0.4225 - val_accuracy: 0.8714\n",
      "Epoch 6/10\n",
      "600/600 - 25s - loss: 0.6013 - accuracy: 0.8099 - val_loss: 0.3501 - val_accuracy: 0.8985\n",
      "Epoch 7/10\n",
      "600/600 - 25s - loss: 0.5559 - accuracy: 0.8218 - val_loss: 0.3520 - val_accuracy: 0.9001\n",
      "Epoch 8/10\n",
      "600/600 - 27s - loss: 0.5309 - accuracy: 0.8324 - val_loss: 0.3207 - val_accuracy: 0.9070\n",
      "Epoch 9/10\n",
      "600/600 - 25s - loss: 0.5139 - accuracy: 0.8365 - val_loss: 0.2894 - val_accuracy: 0.9196\n",
      "Epoch 10/10\n",
      "600/600 - 25s - loss: 0.5096 - accuracy: 0.8387 - val_loss: 0.2887 - val_accuracy: 0.9201\n",
      "Epoch 1/10\n",
      "600/600 - 25s - loss: 0.5382 - accuracy: 0.8288 - val_loss: 0.3601 - val_accuracy: 0.8973\n",
      "Epoch 2/10\n",
      "600/600 - 25s - loss: 0.4913 - accuracy: 0.8428 - val_loss: 0.3050 - val_accuracy: 0.9086\n",
      "Epoch 3/10\n",
      "600/600 - 25s - loss: 0.4554 - accuracy: 0.8552 - val_loss: 0.2437 - val_accuracy: 0.9273\n",
      "Epoch 4/10\n",
      "600/600 - 25s - loss: 0.4283 - accuracy: 0.8652 - val_loss: 0.2345 - val_accuracy: 0.9321\n",
      "Epoch 5/10\n",
      "600/600 - 25s - loss: 0.4095 - accuracy: 0.8725 - val_loss: 0.2473 - val_accuracy: 0.9282\n",
      "Epoch 6/10\n",
      "600/600 - 25s - loss: 0.3916 - accuracy: 0.8755 - val_loss: 0.2256 - val_accuracy: 0.9379\n",
      "Epoch 7/10\n",
      "600/600 - 27s - loss: 0.3839 - accuracy: 0.8796 - val_loss: 0.1950 - val_accuracy: 0.9443\n",
      "Epoch 8/10\n",
      "600/600 - 25s - loss: 0.3778 - accuracy: 0.8806 - val_loss: 0.2001 - val_accuracy: 0.9434\n",
      "Epoch 9/10\n",
      "600/600 - 25s - loss: 0.3673 - accuracy: 0.8837 - val_loss: 0.1975 - val_accuracy: 0.9434\n",
      "Epoch 10/10\n",
      "600/600 - 25s - loss: 0.3637 - accuracy: 0.8870 - val_loss: 0.1942 - val_accuracy: 0.9442\n",
      "Epoch 1/10\n",
      "600/600 - 26s - loss: 0.3767 - accuracy: 0.8812 - val_loss: 0.2326 - val_accuracy: 0.9339\n",
      "Epoch 2/10\n",
      "600/600 - 25s - loss: 0.3689 - accuracy: 0.8836 - val_loss: 0.2238 - val_accuracy: 0.9348\n",
      "Epoch 3/10\n",
      "600/600 - 25s - loss: 0.3513 - accuracy: 0.8897 - val_loss: 0.1867 - val_accuracy: 0.9455\n",
      "Epoch 4/10\n",
      "600/600 - 25s - loss: 0.3415 - accuracy: 0.8927 - val_loss: 0.1953 - val_accuracy: 0.9452\n",
      "Epoch 5/10\n",
      "600/600 - 27s - loss: 0.3299 - accuracy: 0.8961 - val_loss: 0.1787 - val_accuracy: 0.9486\n",
      "Epoch 6/10\n",
      "600/600 - 25s - loss: 0.3197 - accuracy: 0.9002 - val_loss: 0.1716 - val_accuracy: 0.9519\n",
      "Epoch 7/10\n",
      "600/600 - 25s - loss: 0.3175 - accuracy: 0.9001 - val_loss: 0.1696 - val_accuracy: 0.9551\n",
      "Epoch 8/10\n",
      "600/600 - 25s - loss: 0.3143 - accuracy: 0.9005 - val_loss: 0.1676 - val_accuracy: 0.9526\n",
      "Epoch 9/10\n",
      "600/600 - 25s - loss: 0.3103 - accuracy: 0.9020 - val_loss: 0.1624 - val_accuracy: 0.9536\n",
      "Epoch 10/10\n",
      "600/600 - 25s - loss: 0.3142 - accuracy: 0.9002 - val_loss: 0.1634 - val_accuracy: 0.9533\n",
      "Epoch 1/10\n",
      "600/600 - 25s - loss: 0.3199 - accuracy: 0.8999 - val_loss: 0.1763 - val_accuracy: 0.9505\n",
      "Epoch 2/10\n",
      "600/600 - 25s - loss: 0.3187 - accuracy: 0.9002 - val_loss: 0.1709 - val_accuracy: 0.9523\n",
      "Epoch 3/10\n",
      "600/600 - 27s - loss: 0.3063 - accuracy: 0.9030 - val_loss: 0.1637 - val_accuracy: 0.9537\n",
      "Epoch 4/10\n",
      "600/600 - 25s - loss: 0.2963 - accuracy: 0.9079 - val_loss: 0.1552 - val_accuracy: 0.9547\n",
      "Epoch 5/10\n",
      "600/600 - 25s - loss: 0.2841 - accuracy: 0.9103 - val_loss: 0.1522 - val_accuracy: 0.9576\n",
      "Epoch 6/10\n",
      "600/600 - 25s - loss: 0.2831 - accuracy: 0.9112 - val_loss: 0.1454 - val_accuracy: 0.9580\n",
      "Epoch 7/10\n",
      "600/600 - 25s - loss: 0.2849 - accuracy: 0.9125 - val_loss: 0.1467 - val_accuracy: 0.9584\n",
      "Epoch 8/10\n",
      "600/600 - 25s - loss: 0.2867 - accuracy: 0.9102 - val_loss: 0.1473 - val_accuracy: 0.9582\n",
      "Epoch 9/10\n",
      "600/600 - 25s - loss: 0.2771 - accuracy: 0.9127 - val_loss: 0.1483 - val_accuracy: 0.9582\n",
      "Epoch 10/10\n",
      "600/600 - 25s - loss: 0.2793 - accuracy: 0.9119 - val_loss: 0.1473 - val_accuracy: 0.9582\n",
      "Epoch 1/10\n",
      "600/600 - 25s - loss: 0.2878 - accuracy: 0.9084 - val_loss: 0.1714 - val_accuracy: 0.9522\n",
      "Epoch 2/10\n",
      "600/600 - 25s - loss: 0.2835 - accuracy: 0.9095 - val_loss: 0.1453 - val_accuracy: 0.9585\n",
      "Epoch 3/10\n",
      "600/600 - 27s - loss: 0.2759 - accuracy: 0.9122 - val_loss: 0.1396 - val_accuracy: 0.9600\n",
      "Epoch 4/10\n",
      "600/600 - 25s - loss: 0.2734 - accuracy: 0.9152 - val_loss: 0.1449 - val_accuracy: 0.9597\n",
      "Epoch 5/10\n",
      "600/600 - 25s - loss: 0.2686 - accuracy: 0.9151 - val_loss: 0.1384 - val_accuracy: 0.9613\n",
      "Epoch 6/10\n",
      "600/600 - 25s - loss: 0.2676 - accuracy: 0.9139 - val_loss: 0.1365 - val_accuracy: 0.9612\n",
      "Epoch 7/10\n",
      "600/600 - 25s - loss: 0.2654 - accuracy: 0.9168 - val_loss: 0.1355 - val_accuracy: 0.9609\n",
      "Epoch 8/10\n",
      "600/600 - 27s - loss: 0.2558 - accuracy: 0.9177 - val_loss: 0.1337 - val_accuracy: 0.9614\n",
      "Epoch 9/10\n",
      "600/600 - 25s - loss: 0.2588 - accuracy: 0.9192 - val_loss: 0.1358 - val_accuracy: 0.9620\n",
      "Epoch 10/10\n",
      "600/600 - 27s - loss: 0.2550 - accuracy: 0.9194 - val_loss: 0.1352 - val_accuracy: 0.9614\n",
      "Epoch 1/10\n",
      "600/600 - 25s - loss: 0.2633 - accuracy: 0.9180 - val_loss: 0.1347 - val_accuracy: 0.9608\n",
      "Epoch 2/10\n",
      "600/600 - 25s - loss: 0.2582 - accuracy: 0.9176 - val_loss: 0.1391 - val_accuracy: 0.9610\n",
      "Epoch 3/10\n",
      "600/600 - 25s - loss: 0.2522 - accuracy: 0.9210 - val_loss: 0.1335 - val_accuracy: 0.9624\n",
      "Epoch 4/10\n",
      "600/600 - 25s - loss: 0.2520 - accuracy: 0.9205 - val_loss: 0.1367 - val_accuracy: 0.9615\n",
      "Epoch 5/10\n",
      "600/600 - 25s - loss: 0.2526 - accuracy: 0.9211 - val_loss: 0.1298 - val_accuracy: 0.9626\n",
      "Epoch 6/10\n",
      "600/600 - 25s - loss: 0.2422 - accuracy: 0.9239 - val_loss: 0.1270 - val_accuracy: 0.9636\n",
      "Epoch 7/10\n",
      "600/600 - 27s - loss: 0.2429 - accuracy: 0.9242 - val_loss: 0.1312 - val_accuracy: 0.9627\n",
      "Epoch 8/10\n",
      "600/600 - 25s - loss: 0.2460 - accuracy: 0.9221 - val_loss: 0.1296 - val_accuracy: 0.9638\n",
      "Epoch 9/10\n",
      "600/600 - 27s - loss: 0.2425 - accuracy: 0.9239 - val_loss: 0.1291 - val_accuracy: 0.9634\n",
      "Epoch 10/10\n",
      "600/600 - 26s - loss: 0.2427 - accuracy: 0.9235 - val_loss: 0.1274 - val_accuracy: 0.9633\n",
      "Epoch 1/10\n",
      "600/600 - 26s - loss: 0.2480 - accuracy: 0.9219 - val_loss: 0.1335 - val_accuracy: 0.9627\n",
      "Epoch 2/10\n",
      "600/600 - 25s - loss: 0.2421 - accuracy: 0.9241 - val_loss: 0.1266 - val_accuracy: 0.9630\n",
      "Epoch 3/10\n",
      "600/600 - 26s - loss: 0.2390 - accuracy: 0.9255 - val_loss: 0.1275 - val_accuracy: 0.9635\n",
      "Epoch 4/10\n",
      "600/600 - 26s - loss: 0.2412 - accuracy: 0.9222 - val_loss: 0.1266 - val_accuracy: 0.9633\n",
      "Epoch 5/10\n",
      "600/600 - 25s - loss: 0.2403 - accuracy: 0.9245 - val_loss: 0.1252 - val_accuracy: 0.9638\n",
      "Epoch 6/10\n",
      "600/600 - 25s - loss: 0.2326 - accuracy: 0.9257 - val_loss: 0.1228 - val_accuracy: 0.9640\n",
      "Epoch 7/10\n",
      "600/600 - 26s - loss: 0.2296 - accuracy: 0.9270 - val_loss: 0.1255 - val_accuracy: 0.9640\n",
      "Epoch 8/10\n",
      "600/600 - 25s - loss: 0.2277 - accuracy: 0.9290 - val_loss: 0.1239 - val_accuracy: 0.9647\n",
      "Epoch 9/10\n",
      "600/600 - 26s - loss: 0.2281 - accuracy: 0.9287 - val_loss: 0.1242 - val_accuracy: 0.9644\n",
      "Epoch 10/10\n",
      "600/600 - 26s - loss: 0.2322 - accuracy: 0.9266 - val_loss: 0.1221 - val_accuracy: 0.9643\n",
      "Epoch 1/10\n",
      "600/600 - 25s - loss: 0.2337 - accuracy: 0.9262 - val_loss: 0.1309 - val_accuracy: 0.9635\n",
      "Epoch 2/10\n",
      "600/600 - 25s - loss: 0.2289 - accuracy: 0.9273 - val_loss: 0.1238 - val_accuracy: 0.9643\n",
      "Epoch 3/10\n",
      "600/600 - 26s - loss: 0.2312 - accuracy: 0.9272 - val_loss: 0.1216 - val_accuracy: 0.9643\n",
      "Epoch 4/10\n",
      "600/600 - 26s - loss: 0.2235 - accuracy: 0.9304 - val_loss: 0.1221 - val_accuracy: 0.9666\n",
      "Epoch 5/10\n",
      "600/600 - 25s - loss: 0.2246 - accuracy: 0.9287 - val_loss: 0.1181 - val_accuracy: 0.9657\n",
      "Epoch 6/10\n",
      "600/600 - 25s - loss: 0.2220 - accuracy: 0.9302 - val_loss: 0.1208 - val_accuracy: 0.9652\n",
      "Epoch 7/10\n",
      "600/600 - 27s - loss: 0.2197 - accuracy: 0.9310 - val_loss: 0.1196 - val_accuracy: 0.9658\n",
      "Epoch 8/10\n",
      "600/600 - 26s - loss: 0.2156 - accuracy: 0.9308 - val_loss: 0.1177 - val_accuracy: 0.9667\n",
      "Epoch 9/10\n",
      "600/600 - 26s - loss: 0.2131 - accuracy: 0.9315 - val_loss: 0.1211 - val_accuracy: 0.9662\n",
      "Epoch 10/10\n",
      "600/600 - 27s - loss: 0.2185 - accuracy: 0.9315 - val_loss: 0.1188 - val_accuracy: 0.9664\n",
      "(7194, 40)\n",
      "(7194,)\n",
      "Testing closed accuracy_without_norm: 0.9626077286627746\n",
      "(80,)\n",
      "[0.08       0.07804226 0.07236068 0.06351141 0.05236068 0.04\n",
      " 0.02763932 0.01648859 0.00763932 0.00195774]\n",
      "(80,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "#from Model_DF import DFNet\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "snapshot_number=8\n",
    "\n",
    "#MyAdamW = extend_with_decoupled_weight_decay(Adam)\n",
    "# Create a MyAdamW object\n",
    "#OPTIMIZER = MyAdamW(weight_decay=0.001, learning_rate=0.0001,beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# ...\n",
    "learning_rate_data=[]\n",
    "loss_data=[]\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "   initial_lrate = 0.08\n",
    "   \n",
    "   if epoch<2 and i>0:\n",
    "     lrate=0.1\n",
    "   \n",
    "   else:\n",
    "     lrate = initial_lrate * (0.5+0.5*np.cos(epoch*math.pi/(NB_EPOCH//snapshot_number)))\n",
    "\n",
    "   learning_rate_data.append(lrate)\n",
    "   optimizer = model.optimizer\n",
    "   optimizer.lr = lrate\n",
    "   return lrate\n",
    "   \n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "description = \"Training and evaluating DF model for closed-world scenario on non-defended dataset\"\n",
    "\n",
    "print (description)\n",
    "# Training the DF model\n",
    "NB_EPOCH = 80   # Number of training epoch\n",
    "print (\"Number of Epoch: \", NB_EPOCH)\n",
    "BATCH_SIZE = 70 # Batch size\n",
    "VERBOSE = 2 # Output display mode\n",
    "LENGTH = 475 # Packet sequence length\n",
    "OPTIMIZER = Adamax(learning_rate=0.05, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.003) # Optimizer\n",
    "\n",
    "NB_CLASSES = 40 # number of outputs = number of classes\n",
    "INPUT_SHAPE = (LENGTH,1)\n",
    "\n",
    "\n",
    "# Data: shuffled and split between train and test sets\n",
    "\n",
    "print (\"Loading and preparing data for training, and evaluating the model\")\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = LoadDataIot()\n",
    "# Please refer to the dataset format in readme\n",
    "\n",
    "# Convert data as float32 type\n",
    "X_train = X_train.astype('float32')\n",
    "X_valid = X_valid.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_valid = y_valid.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "# we need a [Length x 1] x n shape as input to the DF CNN (Tensorflow)\n",
    "X_train = X_train[:, :,np.newaxis]\n",
    "X_valid = X_valid[:, :,np.newaxis]\n",
    "X_test = X_test[:, :,np.newaxis]\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_valid.shape[0], 'validation samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to categorical classes matrices\n",
    "y_train = to_categorical(y_train, NB_CLASSES)\n",
    "y_valid = to_categorical(y_valid, NB_CLASSES)\n",
    "#y_test = to_categorical(y_test, NB_CLASSES)\n",
    "# Building and training model\n",
    "# print (\"Building and training DF model\")\n",
    "\n",
    "model = DFNet_Add_Layer.build(input_shape=INPUT_SHAPE, nb_classes=NB_CLASSES)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "print (\"Model compiled\")\n",
    "\n",
    "filepath = 'IoT.hdf5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint,lrate]\n",
    "\n",
    "Predictions=[]\n",
    "# Start training\n",
    "for i in range(snapshot_number):\n",
    "  # callbacks_list.append(EarlyStopping(monitor='val_acc', mode='max', patience=6))\n",
    "  #model.summary()\n",
    "  \n",
    "  history = model.fit(X_train, y_train,batch_size=BATCH_SIZE, epochs=NB_EPOCH//snapshot_number,verbose=VERBOSE, validation_data=(X_valid, y_valid), callbacks=callbacks_list)\n",
    "  losss=history.history['val_loss']\n",
    "  #print(losss.shape)\n",
    "  loss_data+=losss\n",
    "  \n",
    "  Predictions.append(model.predict(X_test))\n",
    "  \n",
    "  model2 = Model(model.input, model.layers[-2].output)\n",
    "  model2.save('snapshot_'+str(i)+'.hdf5')\n",
    "\n",
    "\n",
    "#model=load_model('AWF.hdf5')\n",
    "\n",
    "# Start evaluating model with testing data\n",
    "Predictions=np.array(Predictions)\n",
    "Avg_prediction=np.average(Predictions,axis=0)\n",
    "print(Avg_prediction.shape)\n",
    "\n",
    "\n",
    "Avg_prediction=np.argmax(Avg_prediction,axis=1)\n",
    "print(Avg_prediction.shape)\n",
    "        \n",
    "#print(\"############## Training is Done Successfully ###################\")\n",
    "#model.save('DC_without_norm.hdf5')\n",
    "\n",
    "#model=load_model('AWF.hdf5')\n",
    "\n",
    "# Start evaluating model with testing data\n",
    "score_test = accuracy_score(Avg_prediction, y_test)\n",
    "print(\"Testing closed accuracy_without_norm:\", score_test)\n",
    "\n",
    "lr=history.history['lr']\n",
    "learning_rate_data=np.array(learning_rate_data)\n",
    "print(learning_rate_data.shape)\n",
    "print(learning_rate_data[:10])\n",
    "\n",
    "loss_data=np.array(loss_data)\n",
    "print(loss_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52927078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T20:40:54.906188Z",
     "iopub.status.busy": "2022-06-21T20:40:54.905900Z",
     "iopub.status.idle": "2022-06-21T20:42:06.662934Z",
     "shell.execute_reply": "2022-06-21T20:42:06.661570Z"
    },
    "papermill": {
     "duration": 71.78591,
     "end_time": "2022-06-21T20:42:06.664974",
     "exception": false,
     "start_time": "2022-06-21T20:40:54.879064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions:\n",
      "X: Training data's shape :  (41960, 475)\n",
      "y: Training data's shape :  (41960,)\n",
      "X: Validation data's shape :  (10790, 475)\n",
      "y: Validation data's shape :  (10790,)\n",
      "X: Testing data's shape :  (7194, 475)\n",
      "y: Testing data's shape :  (7194,)\n",
      "41960 train samples\n",
      "10790 validation samples\n",
      "7194 test samples\n",
      "Shape:  (41960, 40)\n",
      "Shape:  (10790, 40)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_valid, y_valid, X_test, y_test = LoadDataIot()\n",
    "# Please refer to the dataset format in readme\n",
    "\n",
    "# Convert data as float32 type\n",
    "X_train = X_train.astype('float32')\n",
    "X_valid = X_valid.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('int16')\n",
    "y_valid = y_valid.astype('int16')\n",
    "y_test = y_test.astype('int16')\n",
    "# we need a [Length x 1] x n shape as input to the DF CNN (Tensorflow)\n",
    "X_train = X_train[:, :,np.newaxis]\n",
    "X_valid = X_valid[:, :,np.newaxis]\n",
    "X_test = X_test[:, :,np.newaxis]\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_valid.shape[0], 'validation samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "#y_test_=np.argmax(y_test_, axis=1)\n",
    "txt_O = \"Mean_{Class1:.0f}\"\n",
    "Means={}\n",
    "for i in range(NB_CLASSES):\n",
    "  Means[txt_O.format(Class1=i)]=np.array([0]*NB_CLASSES)\n",
    "  \n",
    "dataset_dir='./'\n",
    "\n",
    "tflite_model_predictions=[]\n",
    "for i in range(snapshot_number):\n",
    "  model=load_model(dataset_dir+'snapshot_'+str(i)+'.hdf5')\n",
    "  tflite_model_predictions.append(model.predict(X_train))\n",
    "  \n",
    "tflite_model_predictions=np.array(tflite_model_predictions)\n",
    "tflite_model_predictions=np.average(tflite_model_predictions,axis=0)\n",
    "\n",
    "\n",
    "print('Shape: ',tflite_model_predictions.shape)\n",
    "count=[0]*NB_CLASSES\n",
    "\n",
    "for i in range(len(tflite_model_predictions)):\n",
    "  k=np.argmax(tflite_model_predictions[i])\n",
    "  if (np.argmax(tflite_model_predictions[i])==y_train[i]):\n",
    "    Means[txt_O.format(Class1=y_train[i])]=Means[txt_O.format(Class1=y_train[i])]+tflite_model_predictions[i]\n",
    "    count[y_train[i]]+=1\n",
    "#print(\"Counts: \",count)\n",
    "\n",
    "Mean_Vectors=[]   \n",
    "for i in range(NB_CLASSES):\n",
    "  Means[txt_O.format(Class1=i)]=Means[txt_O.format(Class1=i)]/count[i]\n",
    "  Mean_Vectors.append(Means[txt_O.format(Class1=i)])\n",
    "\n",
    "Mean_vectors=np.array(Mean_Vectors)\n",
    "np.save('Mean_vectors.npy', Mean_vectors, allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "tflite_model_predictions=[]\n",
    "for i in range(snapshot_number):\n",
    "  model=load_model(dataset_dir+'snapshot_'+str(i)+'.hdf5')\n",
    "  tflite_model_predictions.append(model.predict(X_valid))\n",
    "    \n",
    "tflite_model_predictions=np.array(tflite_model_predictions)\n",
    "tflite_model_predictions=np.average(tflite_model_predictions,axis=0)\n",
    "\n",
    "\n",
    "print('Shape: ',tflite_model_predictions.shape)\n",
    "\n",
    "txt_1 = \"Dist_{Class1:.0f}\"\n",
    "Distances={}\n",
    "for i in range(NB_CLASSES):\n",
    "  Distances[txt_1.format(Class1=i)]=[]\n",
    "  \n",
    "  \n",
    "for i in range(len(tflite_model_predictions)):\n",
    "  if (y_valid[i]==np.argmax(tflite_model_predictions[i])):\n",
    "    dist = np.linalg.norm(Mean_Vectors[y_valid[i]]-tflite_model_predictions[i])\n",
    "    Distances[txt_1.format(Class1=y_valid[i])].append(dist)\n",
    "\n",
    "#print(Distances)    \n",
    "TH=[0]*NB_CLASSES  \n",
    "for j in range(NB_CLASSES):\n",
    "  Distances[txt_1.format(Class1=j)].sort()\n",
    "  Dist=Distances[txt_1.format(Class1=j)]\n",
    "  TH[j]=Dist[int(len(Dist)*0.90)]  \n",
    "\n",
    "\n",
    "\n",
    "Threasholds=np.array(TH)\n",
    "#print(Threasholds)\n",
    "np.save('Threasholds_s.npy',Threasholds)\n",
    "\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2849199",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T20:42:06.717417Z",
     "iopub.status.busy": "2022-06-21T20:42:06.717126Z",
     "iopub.status.idle": "2022-06-21T20:44:08.753272Z",
     "shell.execute_reply": "2022-06-21T20:44:08.752237Z"
    },
    "papermill": {
     "duration": 122.091025,
     "end_time": "2022-06-21T20:44:08.781744",
     "exception": false,
     "start_time": "2022-06-21T20:42:06.690719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_open shape: (86797, 475)\n",
      "\n",
      "Test accuracy TFLITE model_Closed_set : 0.8577981651376146\n",
      "Test accuracy TFLITE model_Open_set : 0.5954929317833566\n",
      "Precision:  0.11652959621233752\n",
      "Recall:  0.8637789753277099\n",
      "Average F1_Score:  0.20535536892580533\n",
      "\n",
      "Micro\n",
      "Test accuracy TFLITE model_Closed_set : 0.8577981651376146\n",
      "Test accuracy TFLITE model_Open_set : 0.5954929317833566\n",
      "Micro Precision:  0.1489428461092875\n",
      "Micro Recall:  0.8577981651376146\n",
      "Average F1_Score:  0.25381482905043407\n",
      "\n",
      "Macro\n",
      "Test accuracy TFLITE model_Closed_set : 0.8577981651376146\n",
      "Test accuracy TFLITE model_Open_set : 0.5954929317833566\n",
      "Precision: 0.23673931529840697\n",
      "Recall: 0.857742406110723\n",
      "Average F1_Score:  0.37106393665177406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train_Rep).batch(1).take(100):\n",
    "    # Model has only one input so each data point has one element.\n",
    "    yield [input_value]\n",
    "    \n",
    "def Macro_F1(Matrix):\n",
    "  Precisions=np.zeros(NB_CLASSES)\n",
    "  Recalls=np.zeros(NB_CLASSES)\n",
    "  \n",
    "  epsilon=1e-8\n",
    "  \n",
    "  for k in range(len(Precisions)):\n",
    "    Precisions[k]=Matrix[k][k]/np.sum(Matrix,axis=0)[k]\n",
    "  #print(Precisions)\n",
    "    \n",
    "  Precision=np.average(Precisions)\n",
    "  print(\"Precision:\",Precision)\n",
    "    \n",
    "  for k in range(len(Recalls)):\n",
    "    Recalls[k]=Matrix[k][k]/np.sum(Matrix,axis=1)[k]\n",
    "   \n",
    "  Recall=np.average(Recalls)\n",
    "  print(\"Recall:\",Recall)\n",
    "\n",
    "  F1_Score=2*Precision*Recall/(Precision+Recall+epsilon)\n",
    "  return F1_Score\n",
    "    \n",
    "def Micro_F1(matrix):\n",
    "  epsilon=1e-8\n",
    "  TP=0\n",
    "  FP=0\n",
    "  TN=0\n",
    "  \n",
    "  for k in range(NB_CLASSES):\n",
    "    TP+=matrix[k][k]\n",
    "    FP+=(np.sum(Matrix,axis=0)[k]-matrix[k][k])\n",
    "    TN+=(np.sum(Matrix,axis=1)[k]-matrix[k][k])\n",
    "    \n",
    "  Micro_Prec=TP/(TP+FP)\n",
    "  Micro_Rec=TP/(TP+TN)\n",
    "  print(\"Micro Precision: \", Micro_Prec)\n",
    "  print(\"Micro Recall: \", Micro_Rec)\n",
    "  Micro_F1=2*Micro_Prec*Micro_Rec/(Micro_Rec+Micro_Prec+epsilon)\n",
    "  \n",
    "  return Micro_F1\n",
    "\n",
    "\n",
    "def New_F1_Score(Matrix):\n",
    "  Column_sum=np.sum(Matrix,axis=0)\n",
    "  Raw_sum=np.sum(Matrix,axis=1)\n",
    "  \n",
    "  Precision_Differences=[]\n",
    "  Recall_Differences=[]\n",
    "  for i in range(NB_CLASSES):\n",
    "    Precision_Differences.append(np.abs(2*Matrix[i][i]-Column_sum[i]))\n",
    "    Recall_Differences.append(np.abs(2*Matrix[i][i]-Raw_sum[i]))\n",
    "  \n",
    "  Precision_Differences=np.array(Precision_Differences)\n",
    "  Precision_Differences_Per=Precision_Differences/np.sum(Precision_Differences)\n",
    "  Recall_Differences=np.array(Recall_Differences)\n",
    "  Recall_Differences_Per=Recall_Differences/np.sum(Recall_Differences)\n",
    "  \n",
    "  #print('Precision_Differences_Per',Precision_Differences_Per)\n",
    "  #print('Recall_Differences_Per',Recall_Differences_Per)\n",
    "  \n",
    "  Precisions=np.zeros(NB_CLASSES)\n",
    "  Recalls=np.zeros(NB_CLASSES)\n",
    "  \n",
    "  epsilon=1e-8\n",
    "  \n",
    "  for k in range(len(Precisions)):\n",
    "    Precisions[k]=(Matrix[k][k]/np.sum(Matrix,axis=0)[k])\n",
    "  #print(Precisions)\n",
    "  Precision=np.sum(np.array(Precisions)*Precision_Differences_Per)\n",
    "  \n",
    "  for k in range(len(Recalls)):\n",
    "    Recalls[k]=(Matrix[k][k]/np.sum(Matrix,axis=1)[k])  #*Recall_Differences_Per[k]\n",
    "  Recall=np.sum(np.array(Recalls)*Recall_Differences_Per)\n",
    "  \n",
    "  print('Precision: ',Precision)\n",
    "  print('Recall: ',Recall)\n",
    "    \n",
    "  \n",
    "  F1_Score=2*Precision*Recall/(Precision+Recall+epsilon)\n",
    "  return F1_Score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_open=np.load(dataset_dir+'X_open.npy')\n",
    "X_open=X_open[:,:1500]\n",
    "y_open = np.array([NB_CLASSES]*len(X_open))\n",
    "\n",
    "X_test,y_test=shuffle(X_test, y_test)\n",
    "#X_open,y_open=shuffle(X_open, y_open)\n",
    "\n",
    "#X_open=X_open[:5000]\n",
    "#y_open=y_open[:5000]\n",
    "\n",
    "\n",
    "print(\"X_open shape:\", X_open.shape)\n",
    "#print(X_test[0])\n",
    "\n",
    "\n",
    "# Convert data as float32 type\n",
    "#X_train = X_train.astype('float32')\n",
    "#X_valid = X_valid.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_open = X_open.astype('float32')\n",
    "#y_train = y_train.astype('float32')\n",
    "#y_valid = y_valid.astype('float32')\n",
    "y_test = y_test.astype('int16')\n",
    "#y_open = y_open.astype('int8')\n",
    "# we need a [Length x 1] x n shape as input to the DF CNN (Tensorflow)\n",
    "#X_train = X_train[:, :,np.newaxis]\n",
    "#X_valid = X_valid[:, :,np.newaxis]\n",
    "\n",
    "\n",
    "print()\n",
    "#print(X_train[0])\n",
    "#print(X_valid[0])\n",
    "#print(\"################################\")\n",
    "#print(X_test[0])\n",
    "\n",
    "#model=load_model('IoT_without_softmax_Robust_Addlayer_s10.hdf5')\n",
    "Mean_Vectors=np.load('Mean_vectors.npy')\n",
    "Threasholds=np.load('Threasholds_s.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_open = X_open[:, :,np.newaxis]\n",
    "\n",
    "\n",
    "prediction_classes=[]\n",
    "tflite_model_predictions=[]\n",
    "for i in range(snapshot_number):\n",
    "  model=load_model('snapshot_'+str(i)+'.hdf5')\n",
    "  tflite_model_predictions.append(model.predict(X_test))\n",
    "  \n",
    "tflite_model_predictions=np.array(tflite_model_predictions)\n",
    "tflite_model_predictions=np.average(tflite_model_predictions,axis=0)\n",
    "\n",
    "for i in range(len(tflite_model_predictions)):\n",
    "  \n",
    "    d=np.argmax(tflite_model_predictions[i], axis=0)\n",
    "    if np.linalg.norm(tflite_model_predictions[i]-Mean_Vectors[d])>Threasholds[d]:\n",
    "      prediction_classes.append(NB_CLASSES)\n",
    "      \n",
    "    else:\n",
    "      prediction_classes.append(d)\n",
    "      \n",
    "\n",
    "    \n",
    "\n",
    "prediction_classes_open=[]\n",
    "tflite_model_predictions_open=[]\n",
    "for i in range(snapshot_number):\n",
    "  model=load_model('snapshot_'+str(i)+'.hdf5')\n",
    "  tflite_model_predictions_open.append(model.predict(X_open))\n",
    "  \n",
    "tflite_model_predictions_open=np.array(tflite_model_predictions_open)\n",
    "tflite_model_predictions_open=np.average(tflite_model_predictions_open,axis=0)\n",
    "\n",
    "for i in range(len(tflite_model_predictions_open)):\n",
    "    d=np.argmax(tflite_model_predictions_open[i], axis=0)\n",
    "    if np.linalg.norm(tflite_model_predictions_open[i]-Mean_Vectors[d])>Threasholds[d]:\n",
    "      prediction_classes_open.append(NB_CLASSES)\n",
    "      \n",
    "    else:\n",
    "      prediction_classes_open.append(d)\n",
    "      \n",
    "    \n",
    "    \n",
    "acc_Close = accuracy_score(prediction_classes, y_test)\n",
    "print('Test accuracy TFLITE model_Closed_set :', acc_Close)\n",
    "\n",
    "acc_Open = accuracy_score(prediction_classes_open, y_open)\n",
    "print('Test accuracy TFLITE model_Open_set :', acc_Open)\n",
    "\n",
    "\n",
    "Matrix=[]\n",
    "for i in range(NB_CLASSES+1):\n",
    "  Matrix.append(np.zeros(NB_CLASSES+1))\n",
    "  \n",
    "  \n",
    "for i in range(len(y_test)):\n",
    "  Matrix[y_test[i]][prediction_classes[i]]+=1\n",
    "  \n",
    "for i in range(len(y_open)):\n",
    "  Matrix[y_open[i]][prediction_classes_open[i]]+=1\n",
    "  \n",
    "#print(Matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "F1_Score=New_F1_Score(Matrix)\n",
    "\n",
    "\n",
    "#print(\"Average Precision: \", Precision)\n",
    "#print(\"Average Recall: \", Recall)\n",
    "print(\"Average F1_Score: \", F1_Score)\n",
    "\n",
    "print()\n",
    "print(\"Micro\")\n",
    "print('Test accuracy TFLITE model_Closed_set :', acc_Close)\n",
    "print('Test accuracy TFLITE model_Open_set :', acc_Open)\n",
    "F1_Score=Micro_F1(Matrix)\n",
    "print(\"Average F1_Score: \", F1_Score)\n",
    "\n",
    "print()\n",
    "print(\"Macro\")\n",
    "print('Test accuracy TFLITE model_Closed_set :', acc_Close)\n",
    "print('Test accuracy TFLITE model_Open_set :', acc_Open)\n",
    "F1_Score=Macro_F1(Matrix)\n",
    "print(\"Average F1_Score: \", F1_Score)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1899b15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T20:44:08.835673Z",
     "iopub.status.busy": "2022-06-21T20:44:08.834140Z",
     "iopub.status.idle": "2022-06-21T20:44:08.840520Z",
     "shell.execute_reply": "2022-06-21T20:44:08.839706Z"
    },
    "papermill": {
     "duration": 0.035216,
     "end_time": "2022-06-21T20:44:08.842554",
     "exception": false,
     "start_time": "2022-06-21T20:44:08.807338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################################################\n"
     ]
    }
   ],
   "source": [
    "print(\"##########################################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72133dba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T20:44:08.895782Z",
     "iopub.status.busy": "2022-06-21T20:44:08.895496Z",
     "iopub.status.idle": "2022-06-21T20:44:22.992819Z",
     "shell.execute_reply": "2022-06-21T20:44:22.991745Z"
    },
    "papermill": {
     "duration": 14.127138,
     "end_time": "2022-06-21T20:44:22.995339",
     "exception": false,
     "start_time": "2022-06-21T20:44:08.868201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'closed': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], 'open': [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]}\n",
      "(88044, 475)\n",
      "(146741, 475)\n",
      "(146741,)\n",
      "[[ 1.  1.  1. ...  0.  0.  0.]\n",
      " [ 1. -1. -1. ...  0.  0.  0.]\n",
      " [ 1.  1.  1. ...  0.  0.  0.]\n",
      " [-1. -1. -1. ...  0.  0.  0.]\n",
      " [ 1. -1.  1. ...  0.  0.  0.]]\n",
      "Data dimensions:\n",
      "X: Training data's shape :  (41928, 475)\n",
      "X: Validating data's shape :  (10782, 475)\n",
      "X: Testing data's shape :  (7188, 475)\n",
      "X: open data's shape :  (86843, 475)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def Exchange_classes(c1,c2,y):\n",
    "  for i in range(len(y)):\n",
    "    if y[i]==c1:\n",
    "      y[i]=c2\n",
    "      \n",
    "    elif y[i]==c2:\n",
    "      y[i]=c1\n",
    "\n",
    "\n",
    "def balance_dataset(X,y):\n",
    "  Classes=int(np.max(y))+1\n",
    "  k=len(y)//Classes-1\n",
    "  X_=list(np.zeros(k*Classes))\n",
    "  y_=list(np.zeros(k*Classes))\n",
    "  \n",
    "  \n",
    "  for j in range(Classes):\n",
    "    for i in range(k):\n",
    "      y_[i*Classes+j]=j\n",
    "      Index=list(y).index(j)\n",
    "      X_[i*Classes+j]=X[Index]\n",
    "      y[Index]=1000\n",
    "  \n",
    "  y=np.array(y_)\n",
    "  X=np.array(X_)\n",
    "  return X,y\n",
    "  \n",
    "def Filter_close_set(Split_Number,data):\n",
    "  closed_set=data[Split_Number][\"closed\"]\n",
    "  open_set=data[Split_Number][\"open\"]\n",
    "  X_5=[]\n",
    "  X_open=[]\n",
    "  y_5=[]\n",
    "  y_open=[]\n",
    "  for i in range(len(y)):\n",
    "    if y[i] in closed_set:\n",
    "      y_5.append(closed_set.index(y[i]))\n",
    "      X_5.append(X[i])\n",
    "    else:\n",
    "      y_open.append(40)\n",
    "      X_open.append(X[i])\n",
    "      \n",
    "  return X_5,X_open,y_5,y_open\n",
    "  \n",
    "  \n",
    "f = open('../input/sydney-data/IOT_data_file.json')\n",
    "data = json.load(f)\n",
    "print(data[\"1\"])  \n",
    "\n",
    "\n",
    "dataset_dir = \"../input/sydney-data/dataset/\"\n",
    "X_train = np.load(dataset_dir+'X_train.npy')\n",
    "print(X_train.shape)\n",
    "y_train = np.load(dataset_dir+'y_train.npy')\n",
    "#print(list(y_train).count(0),list(y_train).count(1),list(y_train).count(2),list(y_train).count(3),list(y_train).count(4),list(y_train).count(5),list(y_train).count(6),list(y_train).count(7),list(y_train).count(8),list(y_train).count(9))\n",
    "\n",
    "    # Load testing data\n",
    "X_test = np.load(dataset_dir+'X_test.npy')\n",
    "y_test = np.load(dataset_dir+'y_test.npy')\n",
    "\n",
    "\n",
    "    # Load testing data\n",
    "X_valid = np.load(dataset_dir+'X_valid.npy')\n",
    "y_valid = np.load(dataset_dir+'y_valid.npy')\n",
    "\n",
    "X=np.concatenate((X_train, X_test,X_valid), axis=0)\n",
    "y=np.concatenate((y_train, y_test,y_valid), axis=0)\n",
    "\n",
    "  \n",
    "#X,y=balance_dataset(X,y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X[:5])\n",
    "\n",
    "\n",
    "\n",
    "Split_Number=\"8\"\n",
    "  \n",
    "  \n",
    "X_5,X_open,y_5,y_open=Filter_close_set(Split_Number,data)\n",
    "\n",
    "\n",
    "  \n",
    "X_train_5, X_valid_5, y_train_5, y_valid_5 = train_test_split(X_5, y_5, test_size=0.3, shuffle=False)\n",
    "X_valid_5, X_test_5, y_valid_5, y_test_5 = train_test_split(X_valid_5, y_valid_5, test_size=0.4, shuffle=False)\n",
    "\n",
    "X_train_5,y_train_5=shuffle(X_train_5, y_train_5)\n",
    "X_valid_5,y_valid_5=shuffle(X_valid_5, y_valid_5)\n",
    "X_test_5,y_test_5=shuffle(X_test_5, y_test_5)\n",
    "\n",
    "\n",
    "X_train_5=np.array(X_train_5)\n",
    "X_valid_5=np.array(X_valid_5)\n",
    "X_test_5=np.array(X_test_5)\n",
    "y_train_5=np.array(y_train_5)\n",
    "y_valid_5=np.array(y_valid_5)\n",
    "y_test_5=np.array(y_test_5)\n",
    "X_open=np.array(X_open)\n",
    "y_open=np.array(y_open)\n",
    "\n",
    "\n",
    "#print(y_test)\n",
    "#print(\"##############\")\n",
    "#print(y_test_5)\n",
    "\n",
    "print (\"Data dimensions:\")\n",
    "print (\"X: Training data's shape : \", X_train_5.shape)\n",
    "print (\"X: Validating data's shape : \", X_valid_5.shape)\n",
    "print (\"X: Testing data's shape : \", X_test_5.shape)\n",
    "print (\"X: open data's shape : \", X_open.shape)\n",
    "    \n",
    "np.save('X_train_5.npy',X_train_5)\n",
    "np.save('X_valid_5.npy',X_valid_5)\n",
    "np.save('X_test_5.npy',X_test_5)\n",
    "np.save('y_valid_5.npy',y_valid_5)\n",
    "np.save('y_train_5.npy',y_train_5)\n",
    "np.save('y_test_5.npy',y_test_5)\n",
    "np.save('X_open.npy',X_open)\n",
    "np.save('y_open.npy',y_open)\n",
    "\n",
    "#print(y_train_5[:50])\n",
    "#print()\n",
    "#print(y_valid_5[:50])\n",
    "#print()\n",
    "#print(y_test_5[:50])\n",
    "#print()\n",
    "#print(y_open[:50])\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c935816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T20:44:23.049984Z",
     "iopub.status.busy": "2022-06-21T20:44:23.049663Z",
     "iopub.status.idle": "2022-06-21T21:19:01.636428Z",
     "shell.execute_reply": "2022-06-21T21:19:01.635390Z"
    },
    "papermill": {
     "duration": 2078.652392,
     "end_time": "2022-06-21T21:19:01.674196",
     "exception": false,
     "start_time": "2022-06-21T20:44:23.021804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating DF model for closed-world scenario on non-defended dataset\n",
      "Number of Epoch:  80\n",
      "Loading and preparing data for training, and evaluating the model\n",
      "Data dimensions:\n",
      "X: Training data's shape :  (41928, 475)\n",
      "y: Training data's shape :  (41928,)\n",
      "X: Validation data's shape :  (10782, 475)\n",
      "y: Validation data's shape :  (10782,)\n",
      "X: Testing data's shape :  (7188, 475)\n",
      "y: Testing data's shape :  (7188,)\n",
      "41928 train samples\n",
      "10782 validation samples\n",
      "7188 test samples\n",
      "Model compiled\n",
      "Epoch 1/10\n",
      "599/599 - 28s - loss: 1.7261 - accuracy: 0.4642 - val_loss: 1.1798 - val_accuracy: 0.6284\n",
      "Epoch 2/10\n",
      "599/599 - 25s - loss: 1.1439 - accuracy: 0.6361 - val_loss: 0.9522 - val_accuracy: 0.6922\n",
      "Epoch 3/10\n",
      "599/599 - 25s - loss: 0.9173 - accuracy: 0.7082 - val_loss: 0.6990 - val_accuracy: 0.7852\n",
      "Epoch 4/10\n",
      "599/599 - 25s - loss: 0.8047 - accuracy: 0.7470 - val_loss: 0.5341 - val_accuracy: 0.8443\n",
      "Epoch 5/10\n",
      "599/599 - 25s - loss: 0.6928 - accuracy: 0.7818 - val_loss: 0.5525 - val_accuracy: 0.8308\n",
      "Epoch 6/10\n",
      "599/599 - 25s - loss: 0.6350 - accuracy: 0.8019 - val_loss: 0.4148 - val_accuracy: 0.8796\n",
      "Epoch 7/10\n",
      "599/599 - 25s - loss: 0.5944 - accuracy: 0.8146 - val_loss: 0.3713 - val_accuracy: 0.8931\n",
      "Epoch 8/10\n",
      "599/599 - 25s - loss: 0.5648 - accuracy: 0.8241 - val_loss: 0.3643 - val_accuracy: 0.8916\n",
      "Epoch 9/10\n",
      "599/599 - 25s - loss: 0.5572 - accuracy: 0.8289 - val_loss: 0.3423 - val_accuracy: 0.9034\n",
      "Epoch 10/10\n",
      "599/599 - 25s - loss: 0.5463 - accuracy: 0.8286 - val_loss: 0.3405 - val_accuracy: 0.9028\n",
      "Epoch 1/10\n",
      "599/599 - 25s - loss: 0.5679 - accuracy: 0.8223 - val_loss: 0.3594 - val_accuracy: 0.8928\n",
      "Epoch 2/10\n",
      "599/599 - 25s - loss: 0.5241 - accuracy: 0.8364 - val_loss: 0.3097 - val_accuracy: 0.9072\n",
      "Epoch 3/10\n",
      "599/599 - 25s - loss: 0.4953 - accuracy: 0.8448 - val_loss: 0.3319 - val_accuracy: 0.9004\n",
      "Epoch 4/10\n",
      "599/599 - 25s - loss: 0.4631 - accuracy: 0.8558 - val_loss: 0.2912 - val_accuracy: 0.9171\n",
      "Epoch 5/10\n",
      "599/599 - 25s - loss: 0.4409 - accuracy: 0.8624 - val_loss: 0.2726 - val_accuracy: 0.9189\n",
      "Epoch 6/10\n",
      "599/599 - 25s - loss: 0.4239 - accuracy: 0.8675 - val_loss: 0.2526 - val_accuracy: 0.9265\n",
      "Epoch 7/10\n",
      "599/599 - 25s - loss: 0.4128 - accuracy: 0.8702 - val_loss: 0.2448 - val_accuracy: 0.9292\n",
      "Epoch 8/10\n",
      "599/599 - 25s - loss: 0.4092 - accuracy: 0.8742 - val_loss: 0.2417 - val_accuracy: 0.9304\n",
      "Epoch 9/10\n",
      "599/599 - 25s - loss: 0.3936 - accuracy: 0.8763 - val_loss: 0.2356 - val_accuracy: 0.9322\n",
      "Epoch 10/10\n",
      "599/599 - 27s - loss: 0.3986 - accuracy: 0.8766 - val_loss: 0.2363 - val_accuracy: 0.9322\n",
      "Epoch 1/10\n",
      "599/599 - 25s - loss: 0.4182 - accuracy: 0.8708 - val_loss: 0.2454 - val_accuracy: 0.9287\n",
      "Epoch 2/10\n",
      "599/599 - 25s - loss: 0.4008 - accuracy: 0.8744 - val_loss: 0.2348 - val_accuracy: 0.9321\n",
      "Epoch 3/10\n",
      "599/599 - 25s - loss: 0.3851 - accuracy: 0.8807 - val_loss: 0.2257 - val_accuracy: 0.9332\n",
      "Epoch 4/10\n",
      "599/599 - 25s - loss: 0.3693 - accuracy: 0.8835 - val_loss: 0.2320 - val_accuracy: 0.9322\n",
      "Epoch 5/10\n",
      "599/599 - 25s - loss: 0.3606 - accuracy: 0.8862 - val_loss: 0.2122 - val_accuracy: 0.9373\n",
      "Epoch 6/10\n",
      "599/599 - 25s - loss: 0.3533 - accuracy: 0.8903 - val_loss: 0.1995 - val_accuracy: 0.9419\n",
      "Epoch 7/10\n",
      "599/599 - 27s - loss: 0.3481 - accuracy: 0.8923 - val_loss: 0.1978 - val_accuracy: 0.9413\n",
      "Epoch 8/10\n",
      "599/599 - 25s - loss: 0.3361 - accuracy: 0.8950 - val_loss: 0.1977 - val_accuracy: 0.9416\n",
      "Epoch 9/10\n",
      "599/599 - 25s - loss: 0.3361 - accuracy: 0.8947 - val_loss: 0.1982 - val_accuracy: 0.9419\n",
      "Epoch 10/10\n",
      "599/599 - 25s - loss: 0.3353 - accuracy: 0.8959 - val_loss: 0.2009 - val_accuracy: 0.9413\n",
      "Epoch 1/10\n",
      "599/599 - 25s - loss: 0.3476 - accuracy: 0.8906 - val_loss: 0.2075 - val_accuracy: 0.9377\n",
      "Epoch 2/10\n",
      "599/599 - 25s - loss: 0.3411 - accuracy: 0.8938 - val_loss: 0.1980 - val_accuracy: 0.9436\n",
      "Epoch 3/10\n",
      "599/599 - 25s - loss: 0.3388 - accuracy: 0.8942 - val_loss: 0.2083 - val_accuracy: 0.9396\n",
      "Epoch 4/10\n",
      "599/599 - 25s - loss: 0.3211 - accuracy: 0.9011 - val_loss: 0.1941 - val_accuracy: 0.9434\n",
      "Epoch 5/10\n",
      "599/599 - 25s - loss: 0.3151 - accuracy: 0.9006 - val_loss: 0.1859 - val_accuracy: 0.9446\n",
      "Epoch 6/10\n",
      "599/599 - 25s - loss: 0.3162 - accuracy: 0.9019 - val_loss: 0.1788 - val_accuracy: 0.9494\n",
      "Epoch 7/10\n",
      "599/599 - 25s - loss: 0.3072 - accuracy: 0.9030 - val_loss: 0.1816 - val_accuracy: 0.9475\n",
      "Epoch 8/10\n",
      "599/599 - 25s - loss: 0.3056 - accuracy: 0.9053 - val_loss: 0.1767 - val_accuracy: 0.9494\n",
      "Epoch 9/10\n",
      "599/599 - 25s - loss: 0.2997 - accuracy: 0.9068 - val_loss: 0.1819 - val_accuracy: 0.9479\n",
      "Epoch 10/10\n",
      "599/599 - 25s - loss: 0.2974 - accuracy: 0.9065 - val_loss: 0.1797 - val_accuracy: 0.9480\n",
      "Epoch 1/10\n",
      "599/599 - 25s - loss: 0.3101 - accuracy: 0.9039 - val_loss: 0.1802 - val_accuracy: 0.9488\n",
      "Epoch 2/10\n",
      "599/599 - 25s - loss: 0.3026 - accuracy: 0.9050 - val_loss: 0.1881 - val_accuracy: 0.9452\n",
      "Epoch 3/10\n",
      "599/599 - 25s - loss: 0.2995 - accuracy: 0.9072 - val_loss: 0.1769 - val_accuracy: 0.9487\n",
      "Epoch 4/10\n",
      "599/599 - 25s - loss: 0.2912 - accuracy: 0.9090 - val_loss: 0.1725 - val_accuracy: 0.9513\n",
      "Epoch 5/10\n",
      "599/599 - 25s - loss: 0.2871 - accuracy: 0.9096 - val_loss: 0.1751 - val_accuracy: 0.9495\n",
      "Epoch 6/10\n",
      "599/599 - 27s - loss: 0.2790 - accuracy: 0.9144 - val_loss: 0.1769 - val_accuracy: 0.9490\n",
      "Epoch 7/10\n",
      "599/599 - 25s - loss: 0.2862 - accuracy: 0.9103 - val_loss: 0.1655 - val_accuracy: 0.9533\n",
      "Epoch 8/10\n",
      "599/599 - 25s - loss: 0.2787 - accuracy: 0.9141 - val_loss: 0.1682 - val_accuracy: 0.9505\n",
      "Epoch 9/10\n",
      "599/599 - 25s - loss: 0.2820 - accuracy: 0.9127 - val_loss: 0.1692 - val_accuracy: 0.9514\n",
      "Epoch 10/10\n",
      "599/599 - 27s - loss: 0.2786 - accuracy: 0.9134 - val_loss: 0.1686 - val_accuracy: 0.9510\n",
      "Epoch 1/10\n",
      "599/599 - 25s - loss: 0.2875 - accuracy: 0.9100 - val_loss: 0.1868 - val_accuracy: 0.9471\n",
      "Epoch 2/10\n",
      "599/599 - 25s - loss: 0.2810 - accuracy: 0.9114 - val_loss: 0.1616 - val_accuracy: 0.9548\n",
      "Epoch 3/10\n",
      "599/599 - 25s - loss: 0.2754 - accuracy: 0.9144 - val_loss: 0.1634 - val_accuracy: 0.9529\n",
      "Epoch 4/10\n",
      "599/599 - 25s - loss: 0.2739 - accuracy: 0.9146 - val_loss: 0.1651 - val_accuracy: 0.9524\n",
      "Epoch 5/10\n",
      "599/599 - 25s - loss: 0.2688 - accuracy: 0.9166 - val_loss: 0.1597 - val_accuracy: 0.9544\n",
      "Epoch 6/10\n",
      "599/599 - 25s - loss: 0.2650 - accuracy: 0.9171 - val_loss: 0.1573 - val_accuracy: 0.9541\n",
      "Epoch 7/10\n",
      "599/599 - 27s - loss: 0.2601 - accuracy: 0.9177 - val_loss: 0.1562 - val_accuracy: 0.9558\n",
      "Epoch 8/10\n",
      "599/599 - 25s - loss: 0.2659 - accuracy: 0.9170 - val_loss: 0.1553 - val_accuracy: 0.9559\n",
      "Epoch 9/10\n",
      "599/599 - 25s - loss: 0.2587 - accuracy: 0.9193 - val_loss: 0.1548 - val_accuracy: 0.9552\n",
      "Epoch 10/10\n",
      "599/599 - 25s - loss: 0.2640 - accuracy: 0.9172 - val_loss: 0.1555 - val_accuracy: 0.9555\n",
      "Epoch 1/10\n",
      "599/599 - 25s - loss: 0.2679 - accuracy: 0.9164 - val_loss: 0.1622 - val_accuracy: 0.9524\n",
      "Epoch 2/10\n",
      "599/599 - 25s - loss: 0.2649 - accuracy: 0.9185 - val_loss: 0.1644 - val_accuracy: 0.9516\n",
      "Epoch 3/10\n",
      "599/599 - 25s - loss: 0.2629 - accuracy: 0.9155 - val_loss: 0.1533 - val_accuracy: 0.9558\n",
      "Epoch 4/10\n",
      "599/599 - 25s - loss: 0.2611 - accuracy: 0.9183 - val_loss: 0.1581 - val_accuracy: 0.9554\n",
      "Epoch 5/10\n",
      "599/599 - 25s - loss: 0.2565 - accuracy: 0.9190 - val_loss: 0.1503 - val_accuracy: 0.9569\n",
      "Epoch 6/10\n",
      "599/599 - 25s - loss: 0.2515 - accuracy: 0.9224 - val_loss: 0.1552 - val_accuracy: 0.9552\n",
      "Epoch 7/10\n",
      "599/599 - 26s - loss: 0.2569 - accuracy: 0.9205 - val_loss: 0.1506 - val_accuracy: 0.9565\n",
      "Epoch 8/10\n",
      "599/599 - 25s - loss: 0.2505 - accuracy: 0.9217 - val_loss: 0.1490 - val_accuracy: 0.9574\n",
      "Epoch 9/10\n",
      "599/599 - 25s - loss: 0.2517 - accuracy: 0.9211 - val_loss: 0.1491 - val_accuracy: 0.9571\n",
      "Epoch 10/10\n",
      "599/599 - 25s - loss: 0.2437 - accuracy: 0.9256 - val_loss: 0.1476 - val_accuracy: 0.9575\n",
      "Epoch 1/10\n",
      "599/599 - 25s - loss: 0.2516 - accuracy: 0.9217 - val_loss: 0.1506 - val_accuracy: 0.9559\n",
      "Epoch 2/10\n",
      "599/599 - 25s - loss: 0.2532 - accuracy: 0.9214 - val_loss: 0.1638 - val_accuracy: 0.9541\n",
      "Epoch 3/10\n",
      "599/599 - 25s - loss: 0.2493 - accuracy: 0.9231 - val_loss: 0.1507 - val_accuracy: 0.9583\n",
      "Epoch 4/10\n",
      "599/599 - 25s - loss: 0.2479 - accuracy: 0.9235 - val_loss: 0.1510 - val_accuracy: 0.9580\n",
      "Epoch 5/10\n",
      "599/599 - 25s - loss: 0.2468 - accuracy: 0.9231 - val_loss: 0.1432 - val_accuracy: 0.9598\n",
      "Epoch 6/10\n",
      "599/599 - 27s - loss: 0.2438 - accuracy: 0.9227 - val_loss: 0.1450 - val_accuracy: 0.9596\n",
      "Epoch 7/10\n",
      "599/599 - 26s - loss: 0.2378 - accuracy: 0.9254 - val_loss: 0.1461 - val_accuracy: 0.9592\n",
      "Epoch 8/10\n",
      "599/599 - 25s - loss: 0.2375 - accuracy: 0.9263 - val_loss: 0.1426 - val_accuracy: 0.9600\n",
      "Epoch 9/10\n",
      "599/599 - 25s - loss: 0.2355 - accuracy: 0.9271 - val_loss: 0.1461 - val_accuracy: 0.9592\n",
      "Epoch 10/10\n",
      "599/599 - 25s - loss: 0.2370 - accuracy: 0.9254 - val_loss: 0.1447 - val_accuracy: 0.9597\n",
      "(7188, 40)\n",
      "(7188,)\n",
      "Testing closed accuracy_without_norm: 0.9532554257095158\n",
      "(80,)\n",
      "[0.08       0.07804226 0.07236068 0.06351141 0.05236068 0.04\n",
      " 0.02763932 0.01648859 0.00763932 0.00195774]\n",
      "(80,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "#from Model_DF import DFNet\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "snapshot_number=8\n",
    "\n",
    "#MyAdamW = extend_with_decoupled_weight_decay(Adam)\n",
    "# Create a MyAdamW object\n",
    "#OPTIMIZER = MyAdamW(weight_decay=0.001, learning_rate=0.0001,beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# ...\n",
    "learning_rate_data=[]\n",
    "loss_data=[]\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "   initial_lrate = 0.08\n",
    "   \n",
    "   if epoch<2 and i>0:\n",
    "     lrate=0.1\n",
    "   \n",
    "   else:\n",
    "     lrate = initial_lrate * (0.5+0.5*np.cos(epoch*math.pi/(NB_EPOCH//snapshot_number)))\n",
    "\n",
    "   learning_rate_data.append(lrate)\n",
    "   optimizer = model.optimizer\n",
    "   optimizer.lr = lrate\n",
    "   return lrate\n",
    "   \n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "description = \"Training and evaluating DF model for closed-world scenario on non-defended dataset\"\n",
    "\n",
    "print (description)\n",
    "# Training the DF model\n",
    "NB_EPOCH = 80   # Number of training epoch\n",
    "print (\"Number of Epoch: \", NB_EPOCH)\n",
    "BATCH_SIZE = 70 # Batch size\n",
    "VERBOSE = 2 # Output display mode\n",
    "LENGTH = 475 # Packet sequence length\n",
    "OPTIMIZER = Adamax(learning_rate=0.05, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.003) # Optimizer\n",
    "\n",
    "NB_CLASSES = 40 # number of outputs = number of classes\n",
    "INPUT_SHAPE = (LENGTH,1)\n",
    "\n",
    "\n",
    "# Data: shuffled and split between train and test sets\n",
    "\n",
    "print (\"Loading and preparing data for training, and evaluating the model\")\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = LoadDataIot()\n",
    "# Please refer to the dataset format in readme\n",
    "\n",
    "# Convert data as float32 type\n",
    "X_train = X_train.astype('float32')\n",
    "X_valid = X_valid.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_valid = y_valid.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "# we need a [Length x 1] x n shape as input to the DF CNN (Tensorflow)\n",
    "X_train = X_train[:, :,np.newaxis]\n",
    "X_valid = X_valid[:, :,np.newaxis]\n",
    "X_test = X_test[:, :,np.newaxis]\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_valid.shape[0], 'validation samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to categorical classes matrices\n",
    "y_train = to_categorical(y_train, NB_CLASSES)\n",
    "y_valid = to_categorical(y_valid, NB_CLASSES)\n",
    "#y_test = to_categorical(y_test, NB_CLASSES)\n",
    "# Building and training model\n",
    "# print (\"Building and training DF model\")\n",
    "\n",
    "model = DFNet_Add_Layer.build(input_shape=INPUT_SHAPE, nb_classes=NB_CLASSES)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "print (\"Model compiled\")\n",
    "\n",
    "filepath = 'IoT.hdf5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint,lrate]\n",
    "\n",
    "Predictions=[]\n",
    "# Start training\n",
    "for i in range(snapshot_number):\n",
    "  # callbacks_list.append(EarlyStopping(monitor='val_acc', mode='max', patience=6))\n",
    "  #model.summary()\n",
    "  \n",
    "  history = model.fit(X_train, y_train,batch_size=BATCH_SIZE, epochs=NB_EPOCH//snapshot_number,verbose=VERBOSE, validation_data=(X_valid, y_valid), callbacks=callbacks_list)\n",
    "  losss=history.history['val_loss']\n",
    "  #print(losss.shape)\n",
    "  loss_data+=losss\n",
    "  \n",
    "  Predictions.append(model.predict(X_test))\n",
    "  \n",
    "  model2 = Model(model.input, model.layers[-2].output)\n",
    "  model2.save('snapshot_'+str(i)+'.hdf5')\n",
    "\n",
    "\n",
    "#model=load_model('AWF.hdf5')\n",
    "\n",
    "# Start evaluating model with testing data\n",
    "Predictions=np.array(Predictions)\n",
    "Avg_prediction=np.average(Predictions,axis=0)\n",
    "print(Avg_prediction.shape)\n",
    "\n",
    "\n",
    "Avg_prediction=np.argmax(Avg_prediction,axis=1)\n",
    "print(Avg_prediction.shape)\n",
    "        \n",
    "#print(\"############## Training is Done Successfully ###################\")\n",
    "#model.save('DC_without_norm.hdf5')\n",
    "\n",
    "#model=load_model('AWF.hdf5')\n",
    "\n",
    "# Start evaluating model with testing data\n",
    "score_test = accuracy_score(Avg_prediction, y_test)\n",
    "print(\"Testing closed accuracy_without_norm:\", score_test)\n",
    "\n",
    "lr=history.history['lr']\n",
    "learning_rate_data=np.array(learning_rate_data)\n",
    "print(learning_rate_data.shape)\n",
    "print(learning_rate_data[:10])\n",
    "\n",
    "loss_data=np.array(loss_data)\n",
    "print(loss_data.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f57bdb0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T21:19:01.745606Z",
     "iopub.status.busy": "2022-06-21T21:19:01.745322Z",
     "iopub.status.idle": "2022-06-21T21:20:07.909114Z",
     "shell.execute_reply": "2022-06-21T21:20:07.907765Z"
    },
    "papermill": {
     "duration": 66.201768,
     "end_time": "2022-06-21T21:20:07.911328",
     "exception": false,
     "start_time": "2022-06-21T21:19:01.709560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions:\n",
      "X: Training data's shape :  (41928, 475)\n",
      "y: Training data's shape :  (41928,)\n",
      "X: Validation data's shape :  (10782, 475)\n",
      "y: Validation data's shape :  (10782,)\n",
      "X: Testing data's shape :  (7188, 475)\n",
      "y: Testing data's shape :  (7188,)\n",
      "41928 train samples\n",
      "10782 validation samples\n",
      "7188 test samples\n",
      "Shape:  (41928, 40)\n",
      "Shape:  (10782, 40)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_valid, y_valid, X_test, y_test = LoadDataIot()\n",
    "# Please refer to the dataset format in readme\n",
    "\n",
    "# Convert data as float32 type\n",
    "X_train = X_train.astype('float32')\n",
    "X_valid = X_valid.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('int16')\n",
    "y_valid = y_valid.astype('int16')\n",
    "y_test = y_test.astype('int16')\n",
    "# we need a [Length x 1] x n shape as input to the DF CNN (Tensorflow)\n",
    "X_train = X_train[:, :,np.newaxis]\n",
    "X_valid = X_valid[:, :,np.newaxis]\n",
    "X_test = X_test[:, :,np.newaxis]\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_valid.shape[0], 'validation samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "#y_test_=np.argmax(y_test_, axis=1)\n",
    "txt_O = \"Mean_{Class1:.0f}\"\n",
    "Means={}\n",
    "for i in range(NB_CLASSES):\n",
    "  Means[txt_O.format(Class1=i)]=np.array([0]*NB_CLASSES)\n",
    "  \n",
    "dataset_dir='./'\n",
    "\n",
    "tflite_model_predictions=[]\n",
    "for i in range(snapshot_number):\n",
    "  model=load_model(dataset_dir+'snapshot_'+str(i)+'.hdf5')\n",
    "  tflite_model_predictions.append(model.predict(X_train))\n",
    "  \n",
    "tflite_model_predictions=np.array(tflite_model_predictions)\n",
    "tflite_model_predictions=np.average(tflite_model_predictions,axis=0)\n",
    "\n",
    "\n",
    "print('Shape: ',tflite_model_predictions.shape)\n",
    "count=[0]*NB_CLASSES\n",
    "\n",
    "for i in range(len(tflite_model_predictions)):\n",
    "  k=np.argmax(tflite_model_predictions[i])\n",
    "  if (np.argmax(tflite_model_predictions[i])==y_train[i]):\n",
    "    Means[txt_O.format(Class1=y_train[i])]=Means[txt_O.format(Class1=y_train[i])]+tflite_model_predictions[i]\n",
    "    count[y_train[i]]+=1\n",
    "#print(\"Counts: \",count)\n",
    "\n",
    "Mean_Vectors=[]   \n",
    "for i in range(NB_CLASSES):\n",
    "  Means[txt_O.format(Class1=i)]=Means[txt_O.format(Class1=i)]/count[i]\n",
    "  Mean_Vectors.append(Means[txt_O.format(Class1=i)])\n",
    "\n",
    "Mean_vectors=np.array(Mean_Vectors)\n",
    "np.save('Mean_vectors.npy', Mean_vectors, allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "tflite_model_predictions=[]\n",
    "for i in range(snapshot_number):\n",
    "  model=load_model(dataset_dir+'snapshot_'+str(i)+'.hdf5')\n",
    "  tflite_model_predictions.append(model.predict(X_valid))\n",
    "    \n",
    "tflite_model_predictions=np.array(tflite_model_predictions)\n",
    "tflite_model_predictions=np.average(tflite_model_predictions,axis=0)\n",
    "\n",
    "\n",
    "print('Shape: ',tflite_model_predictions.shape)\n",
    "\n",
    "txt_1 = \"Dist_{Class1:.0f}\"\n",
    "Distances={}\n",
    "for i in range(NB_CLASSES):\n",
    "  Distances[txt_1.format(Class1=i)]=[]\n",
    "  \n",
    "  \n",
    "for i in range(len(tflite_model_predictions)):\n",
    "  if (y_valid[i]==np.argmax(tflite_model_predictions[i])):\n",
    "    dist = np.linalg.norm(Mean_Vectors[y_valid[i]]-tflite_model_predictions[i])\n",
    "    Distances[txt_1.format(Class1=y_valid[i])].append(dist)\n",
    "\n",
    "#print(Distances)    \n",
    "TH=[0]*NB_CLASSES  \n",
    "for j in range(NB_CLASSES):\n",
    "  Distances[txt_1.format(Class1=j)].sort()\n",
    "  Dist=Distances[txt_1.format(Class1=j)]\n",
    "  TH[j]=Dist[int(len(Dist)*0.90)]  \n",
    "\n",
    "\n",
    "\n",
    "Threasholds=np.array(TH)\n",
    "#print(Threasholds)\n",
    "np.save('Threasholds_s.npy',Threasholds)\n",
    "\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "902a9996",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T21:20:07.985242Z",
     "iopub.status.busy": "2022-06-21T21:20:07.984954Z",
     "iopub.status.idle": "2022-06-21T21:23:01.280197Z",
     "shell.execute_reply": "2022-06-21T21:23:01.279185Z"
    },
    "papermill": {
     "duration": 173.371075,
     "end_time": "2022-06-21T21:23:01.318720",
     "exception": false,
     "start_time": "2022-06-21T21:20:07.947645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_open shape: (86843, 475)\n",
      "\n",
      "Test accuracy TFLITE model_Closed_set : 0.8505843071786311\n",
      "Test accuracy TFLITE model_Open_set : 0.5403774627776562\n",
      "Precision:  0.1031465866932639\n",
      "Recall:  0.8591235326495368\n",
      "Average F1_Score:  0.18418042343511215\n",
      "\n",
      "Micro\n",
      "Test accuracy TFLITE model_Closed_set : 0.8505843071786311\n",
      "Test accuracy TFLITE model_Open_set : 0.5403774627776562\n",
      "Micro Precision:  0.1322832601311149\n",
      "Micro Recall:  0.8505843071786311\n",
      "Average F1_Score:  0.22895874839606287\n",
      "\n",
      "Macro\n",
      "Test accuracy TFLITE model_Closed_set : 0.8505843071786311\n",
      "Test accuracy TFLITE model_Open_set : 0.5403774627776562\n",
      "Precision: 0.2382960484838764\n",
      "Recall: 0.8495602728215361\n",
      "Average F1_Score:  0.37219410346092785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train_Rep).batch(1).take(100):\n",
    "    # Model has only one input so each data point has one element.\n",
    "    yield [input_value]\n",
    "    \n",
    "def Macro_F1(Matrix):\n",
    "  Precisions=np.zeros(NB_CLASSES)\n",
    "  Recalls=np.zeros(NB_CLASSES)\n",
    "  \n",
    "  epsilon=1e-8\n",
    "  \n",
    "  for k in range(len(Precisions)):\n",
    "    Precisions[k]=Matrix[k][k]/np.sum(Matrix,axis=0)[k]\n",
    "  #print(Precisions)\n",
    "    \n",
    "  Precision=np.average(Precisions)\n",
    "  print(\"Precision:\",Precision)\n",
    "    \n",
    "  for k in range(len(Recalls)):\n",
    "    Recalls[k]=Matrix[k][k]/np.sum(Matrix,axis=1)[k]\n",
    "   \n",
    "  Recall=np.average(Recalls)\n",
    "  print(\"Recall:\",Recall)\n",
    "\n",
    "  F1_Score=2*Precision*Recall/(Precision+Recall+epsilon)\n",
    "  return F1_Score\n",
    "    \n",
    "def Micro_F1(matrix):\n",
    "  epsilon=1e-8\n",
    "  TP=0\n",
    "  FP=0\n",
    "  TN=0\n",
    "  \n",
    "  for k in range(NB_CLASSES):\n",
    "    TP+=matrix[k][k]\n",
    "    FP+=(np.sum(Matrix,axis=0)[k]-matrix[k][k])\n",
    "    TN+=(np.sum(Matrix,axis=1)[k]-matrix[k][k])\n",
    "    \n",
    "  Micro_Prec=TP/(TP+FP)\n",
    "  Micro_Rec=TP/(TP+TN)\n",
    "  print(\"Micro Precision: \", Micro_Prec)\n",
    "  print(\"Micro Recall: \", Micro_Rec)\n",
    "  Micro_F1=2*Micro_Prec*Micro_Rec/(Micro_Rec+Micro_Prec+epsilon)\n",
    "  \n",
    "  return Micro_F1\n",
    "\n",
    "\n",
    "def New_F1_Score(Matrix):\n",
    "  Column_sum=np.sum(Matrix,axis=0)\n",
    "  Raw_sum=np.sum(Matrix,axis=1)\n",
    "  \n",
    "  Precision_Differences=[]\n",
    "  Recall_Differences=[]\n",
    "  for i in range(NB_CLASSES):\n",
    "    Precision_Differences.append(np.abs(2*Matrix[i][i]-Column_sum[i]))\n",
    "    Recall_Differences.append(np.abs(2*Matrix[i][i]-Raw_sum[i]))\n",
    "  \n",
    "  Precision_Differences=np.array(Precision_Differences)\n",
    "  Precision_Differences_Per=Precision_Differences/np.sum(Precision_Differences)\n",
    "  Recall_Differences=np.array(Recall_Differences)\n",
    "  Recall_Differences_Per=Recall_Differences/np.sum(Recall_Differences)\n",
    "  \n",
    "  #print('Precision_Differences_Per',Precision_Differences_Per)\n",
    "  #print('Recall_Differences_Per',Recall_Differences_Per)\n",
    "  \n",
    "  Precisions=np.zeros(NB_CLASSES)\n",
    "  Recalls=np.zeros(NB_CLASSES)\n",
    "  \n",
    "  epsilon=1e-8\n",
    "  \n",
    "  for k in range(len(Precisions)):\n",
    "    Precisions[k]=(Matrix[k][k]/np.sum(Matrix,axis=0)[k])\n",
    "  #print(Precisions)\n",
    "  Precision=np.sum(np.array(Precisions)*Precision_Differences_Per)\n",
    "  \n",
    "  for k in range(len(Recalls)):\n",
    "    Recalls[k]=(Matrix[k][k]/np.sum(Matrix,axis=1)[k])  #*Recall_Differences_Per[k]\n",
    "  Recall=np.sum(np.array(Recalls)*Recall_Differences_Per)\n",
    "  \n",
    "  print('Precision: ',Precision)\n",
    "  print('Recall: ',Recall)\n",
    "    \n",
    "  \n",
    "  F1_Score=2*Precision*Recall/(Precision+Recall+epsilon)\n",
    "  return F1_Score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_open=np.load(dataset_dir+'X_open.npy')\n",
    "X_open=X_open[:,:1500]\n",
    "y_open = np.array([NB_CLASSES]*len(X_open))\n",
    "\n",
    "X_test,y_test=shuffle(X_test, y_test)\n",
    "#X_open,y_open=shuffle(X_open, y_open)\n",
    "\n",
    "#X_open=X_open[:5000]\n",
    "#y_open=y_open[:5000]\n",
    "\n",
    "\n",
    "print(\"X_open shape:\", X_open.shape)\n",
    "#print(X_test[0])\n",
    "\n",
    "\n",
    "# Convert data as float32 type\n",
    "#X_train = X_train.astype('float32')\n",
    "#X_valid = X_valid.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_open = X_open.astype('float32')\n",
    "#y_train = y_train.astype('float32')\n",
    "#y_valid = y_valid.astype('float32')\n",
    "y_test = y_test.astype('int16')\n",
    "#y_open = y_open.astype('int8')\n",
    "# we need a [Length x 1] x n shape as input to the DF CNN (Tensorflow)\n",
    "#X_train = X_train[:, :,np.newaxis]\n",
    "#X_valid = X_valid[:, :,np.newaxis]\n",
    "\n",
    "\n",
    "print()\n",
    "#print(X_train[0])\n",
    "#print(X_valid[0])\n",
    "#print(\"################################\")\n",
    "#print(X_test[0])\n",
    "\n",
    "#model=load_model('IoT_without_softmax_Robust_Addlayer_s10.hdf5')\n",
    "Mean_Vectors=np.load('Mean_vectors.npy')\n",
    "Threasholds=np.load('Threasholds_s.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_open = X_open[:, :,np.newaxis]\n",
    "\n",
    "\n",
    "prediction_classes=[]\n",
    "tflite_model_predictions=[]\n",
    "for i in range(snapshot_number):\n",
    "  model=load_model('snapshot_'+str(i)+'.hdf5')\n",
    "  tflite_model_predictions.append(model.predict(X_test))\n",
    "  \n",
    "tflite_model_predictions=np.array(tflite_model_predictions)\n",
    "tflite_model_predictions=np.average(tflite_model_predictions,axis=0)\n",
    "\n",
    "for i in range(len(tflite_model_predictions)):\n",
    "  \n",
    "    d=np.argmax(tflite_model_predictions[i], axis=0)\n",
    "    if np.linalg.norm(tflite_model_predictions[i]-Mean_Vectors[d])>Threasholds[d]:\n",
    "      prediction_classes.append(NB_CLASSES)\n",
    "      \n",
    "    else:\n",
    "      prediction_classes.append(d)\n",
    "      \n",
    "\n",
    "    \n",
    "\n",
    "prediction_classes_open=[]\n",
    "tflite_model_predictions_open=[]\n",
    "for i in range(snapshot_number):\n",
    "  model=load_model('snapshot_'+str(i)+'.hdf5')\n",
    "  tflite_model_predictions_open.append(model.predict(X_open))\n",
    "  \n",
    "tflite_model_predictions_open=np.array(tflite_model_predictions_open)\n",
    "tflite_model_predictions_open=np.average(tflite_model_predictions_open,axis=0)\n",
    "\n",
    "for i in range(len(tflite_model_predictions_open)):\n",
    "    d=np.argmax(tflite_model_predictions_open[i], axis=0)\n",
    "    if np.linalg.norm(tflite_model_predictions_open[i]-Mean_Vectors[d])>Threasholds[d]:\n",
    "      prediction_classes_open.append(NB_CLASSES)\n",
    "      \n",
    "    else:\n",
    "      prediction_classes_open.append(d)\n",
    "      \n",
    "    \n",
    "    \n",
    "acc_Close = accuracy_score(prediction_classes, y_test)\n",
    "print('Test accuracy TFLITE model_Closed_set :', acc_Close)\n",
    "\n",
    "acc_Open = accuracy_score(prediction_classes_open, y_open)\n",
    "print('Test accuracy TFLITE model_Open_set :', acc_Open)\n",
    "\n",
    "\n",
    "Matrix=[]\n",
    "for i in range(NB_CLASSES+1):\n",
    "  Matrix.append(np.zeros(NB_CLASSES+1))\n",
    "  \n",
    "  \n",
    "for i in range(len(y_test)):\n",
    "  Matrix[y_test[i]][prediction_classes[i]]+=1\n",
    "  \n",
    "for i in range(len(y_open)):\n",
    "  Matrix[y_open[i]][prediction_classes_open[i]]+=1\n",
    "  \n",
    "#print(Matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "F1_Score=New_F1_Score(Matrix)\n",
    "\n",
    "\n",
    "#print(\"Average Precision: \", Precision)\n",
    "#print(\"Average Recall: \", Recall)\n",
    "print(\"Average F1_Score: \", F1_Score)\n",
    "\n",
    "print()\n",
    "print(\"Micro\")\n",
    "print('Test accuracy TFLITE model_Closed_set :', acc_Close)\n",
    "print('Test accuracy TFLITE model_Open_set :', acc_Open)\n",
    "F1_Score=Micro_F1(Matrix)\n",
    "print(\"Average F1_Score: \", F1_Score)\n",
    "\n",
    "print()\n",
    "print(\"Macro\")\n",
    "print('Test accuracy TFLITE model_Closed_set :', acc_Close)\n",
    "print('Test accuracy TFLITE model_Open_set :', acc_Open)\n",
    "F1_Score=Macro_F1(Matrix)\n",
    "print(\"Average F1_Score: \", F1_Score)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326fb05",
   "metadata": {
    "papermill": {
     "duration": 0.035113,
     "end_time": "2022-06-21T21:23:01.388971",
     "exception": false,
     "start_time": "2022-06-21T21:23:01.353858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7027.134192,
   "end_time": "2022-06-21T21:23:05.117278",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-21T19:25:57.983086",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
